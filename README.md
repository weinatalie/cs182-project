# CS182 Project â€” In-Context Learning Across Architectures

## ğŸ”­ Project Vision
Sequence-model experiments and dataset builders to study whether **self-attention is required for In-Context Learning (ICL)**. We compare minimal sequence architectures (RNN/LSTM/SSM-style) to Transformers at similar scale using:

- **Amazon Reviews sentiment prediction** (natural text ICL signal)
- **Large synthetic recursive sequences** (noisy numeric + alphabet encoded)
- **5â€“8 example contexts per sample** (few-shot supervision without parameter updates)

## ğŸ§  Core Hypotheses
1. ICL is a property of **general sequence modeling**, not uniquely self-attention.
2. **Structured recursion + controlled noise** leads models to learn estimators instead of memorizing canonical sequences.
3. Architecture shapes **how an estimator emerges**, not whether ICL works at all.

## ğŸ§ª Key Datasets
We provide both **natural text** and **controlled synthetic** data:

- `amazon_review.csv` â€” raw Amazon reviews for sentiment experiments
- `amazon_reviews_sentiment.csv` â€” cleaned reviews with 5â€“8 context examples per sample for rating prediction
- `noisy_fibonacci.csv/json` â€” numeric noisy Fibonacci-style recursion with Îµ âˆˆ {-2, -1, 0, 1, 2}
- `letter_fibonacci.csv` â€” digitâ†’alphabet encoded Fibonacci (0â†’a, 1â†’b, â€¦, 9â†’j)
- `typo_generator.csv` â€” 100k noise/clean column dataset for typo-correction ICL

## ğŸ“ Repository Layout

cs182-project/
â”œâ”€â”€ data_generator.py # generates synthetic recursive sequences + noise injection
â”œâ”€â”€ data_parse.py # review + synthetic data preprocessing pipeline
â”œâ”€â”€ rnn_gpu.py # RNN experiments with GPU training
â”œâ”€â”€ rnn_reg_gpu.py # RNN regression variant (GPU)
â”œâ”€â”€ transformer_gpu.py # Transformer baseline (GPU)
â”œâ”€â”€ *.csv / *.json # generated or parsed datasets
â””â”€â”€ README.md # this documentation


## âš™ï¸ Environment Setup
Clone and prepare:

```bash
git clone cs182-project
cd cs182-project
python3 -m venv .venv
source .venv/bin/activate
pip install pandas numpy torch tqdm
```

## â–¶ï¸ Usage
Generate synthetic recursive data
```
python data_generator.py --rows 500000 --noise 2 --min_seed -100 --max_seed 100 --out noisy_fibonacci.csv
```
Parse Amazon review or synthetic data
```
python data_parse.py --input amazon_review.csv --output amazon_reviews_processed.csv
```

Train baseline models
```
python transformer_gpu.py --data amazon_reviews_processed.csv --epochs 10 --batch_size 64
python rnn_gpu.py --data amazon_reviews_processed.csv --epochs 10 --batch_size 64
python rnn_reg_gpu.py --data noisy_fibonacci.csv --epochs 15 --batch_size 128
```

## âš ï¸ Limitations

Synthetic data is distribution-controlled, not realism-matched

Few-shot estimator discovery may need manual logging & analysis harness

GPU scripts require a compatible backend like PyTorch

## ğŸ¤ Authors

Research and engineering collaboration by the project team:

Rohan Gulati,
Alena Chao,
Natalie Wei,
Andrew Choy,
Minjune Kim

## ğŸ“š Background References

Scaling and discovery of few-shot ICL was first shown in OpenAI via GPT-3 (Brown et al. 2020)

Estimator emergence in sequence models has been explored using non-attention architectures like RNNs, LSTMs, and SSM families

Controlled recursive-noise datasets are used to avoid memorization while preserving additive structure