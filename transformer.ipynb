{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "700a2b86-3a78-4ec2-91fc-b6d13ad34878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7010da3-3787-4694-9029-daa04d054fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "CUDA device count: 1\n",
      "Current CUDA device name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee70cfe-02a5-40d3-92fc-38b7e81ba116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb5352-2c96-4a53-8a53-20ad585c7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "file = \"amazon_review.csv\"\n",
    "\n",
    "data = pd.read_csv(file).dropna(ignore_index=True)\n",
    "\n",
    "data[\"reviewText\"].to_csv(\"amazon_reviews.txt\", index=False, header=False)\n",
    "data[\"overall\"] = data[\"overall\"] -1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = 16000\n",
    "start_token = 16000\n",
    "end_token = 16001\n",
    "\n",
    "'''\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='amazon_reviews.txt',\n",
    "    model_prefix='amazon_reviews',\n",
    "    vocab_size=vocab_size,\n",
    "    model_type='unigram',\n",
    "    character_coverage=1.0\n",
    ")\n",
    "'''\n",
    "\n",
    "tok = spm.SentencePieceProcessor(model_file='amazon_reviews.model')\n",
    "\n",
    "print(tok.encode(\"This book is amazing!\", out_type=int))\n",
    "\n",
    "ls  = [len(tok.encode(i, out_type=int)) for i in data[\"reviewText\"]]\n",
    "\n",
    "data[\"lengths\"] = ls\n",
    "\n",
    "data_trunc = data[data[\"lengths\"]<=80]\n",
    "\n",
    "v = data_trunc[\"overall\"].value_counts()\n",
    "\n",
    "balanced_data = (\n",
    "    data_trunc.groupby(\"overall\")\n",
    "      .sample(n=min(v), random_state=42)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "filter_ = 1\n",
    "\n",
    "\n",
    "df_shuffled = balanced_data.sample(frac=filter_, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the shuffled DataFrame\n",
    "train_size = 0.8\n",
    "train_df = df_shuffled.sample(frac=train_size, random_state=42).reset_index(drop=True)\n",
    "test_df = df_shuffled.drop(train_df.index).reset_index(drop=True)\n",
    "\n",
    "len_train = len(train_df)\n",
    "len_test = len(test_df)\n",
    "\n",
    "print(\"Train Size: \", len(train_df))\n",
    "print(\"Test Size: \", len(test_df))\n",
    "\n",
    "num_rows = 2000\n",
    "\n",
    "n = [0, 1, 2, 3, 5, 7, 10]\n",
    "def getEncoding(df, i):\n",
    "    row_input = [16000] + tok.encode(df[\"reviewText\"].iloc[i], out_type=int) + [16001]\n",
    "    row_output = tok.encode(str(int(df[\"overall\"].iloc[i])), out_type = int)\n",
    "    return row_input, row_output\n",
    "\n",
    "annos_x = {}\n",
    "for i in n:\n",
    "    annos_x[i]=[]\n",
    "annos_y = []\n",
    "for i in tqdm(range(num_rows)):\n",
    "    k = random.randint(0,len(test_df) - 1)\n",
    "    #input_fin = test_df[\"reviewText\"][k]\n",
    "    #output_fin = test_df[\"overall\"][k]\n",
    "    input_fin, output_fin = getEncoding(test_df, i)\n",
    "    icl_df = test_df.drop(test_df.index[k])\n",
    "    for j in n:\n",
    "        s= icl_df.sample(n=j)\n",
    "        in_ = []\n",
    "        for shot in range(j):\n",
    "            inp, outp = getEncoding(s, shot)\n",
    "            in_.extend(inp)\n",
    "            in_.extend(outp)\n",
    "            \n",
    "            print(\"in: \", inp)\n",
    "            print(str(s[\"overall\"].iloc[shot]))\n",
    "            print(\"out: \", outp)\n",
    "        in_.extend(input_fin)\n",
    "        annos_x[j].append(in_)\n",
    "    annos_y.extend(output_fin)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c632b09e-bcad-4168-99a9-a3dafedd4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 530\n"
     ]
    }
   ],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            emb_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, emb_dim),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        # self-attention only\n",
    "        h = self.ln1(x)\n",
    "        h, _ = self.attn(\n",
    "            h, h, h,\n",
    "            attn_mask=attn_mask,\n",
    "            key_padding_mask=key_padding_mask\n",
    "        )\n",
    "        x = x + self.dropout(h)\n",
    "\n",
    "        # feedforward\n",
    "        h2 = self.ln2(x)\n",
    "        h2 = self.ff(h2)\n",
    "        x = x + self.dropout(h2)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=1024, num_heads=8,\n",
    "                 num_layers=12, ff_dim=512, max_len=800, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.pos_embed = nn.Embedding(max_len, emb_dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(emb_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(emb_dim)\n",
    "        self.fc = nn.Linear(emb_dim, 5)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        B, L = x.shape\n",
    "\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
    "        h = self.embedding(x) + self.pos_embed(pos)\n",
    "\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(L, L, device=x.device), diagonal=1\n",
    "        ).bool()\n",
    "\n",
    "        pad_mask = (x == 0)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            h = block(h, attn_mask=causal_mask, key_padding_mask=pad_mask)\n",
    "\n",
    "        h = self.ln_f(h)\n",
    "\n",
    "        idx = (lengths - 1).view(-1, 1, 1).expand(-1, 1, h.size(-1))\n",
    "        last_hidden = h.gather(1, idx).squeeze(1)\n",
    "\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "\n",
    "    padded = pad_sequence(\n",
    "        sequences,\n",
    "        batch_first=True, \n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    labels = torch.stack([l for l in labels])\n",
    "    return padded, lengths, labels\n",
    "\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, train_dframe, spm_model):\n",
    "        self.df = train_dframe\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(spm_model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"reviewText\"]\n",
    "        label = torch.tensor(self.df.loc[idx, \"overall\"], dtype = torch.long)\n",
    "        ids = self.sp.encode(text, out_type=int)\n",
    "        ids_encoded = [start_token] + ids + [end_token]\n",
    "        ids_encoded = torch.tensor(ids_encoded, dtype=torch.long)\n",
    "        return ids_encoded, label\n",
    "\n",
    "\n",
    "class ICLDataset(Dataset):\n",
    "    def __init__(self, shot_dict, y, shots):\n",
    "        self.ICL_shot = shot_dict[shots]\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ICL_shot)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.ICL_shot[idx]\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        \n",
    "        #label = self.df.loc[idx, \"overall\"].item()\n",
    "        #one_hot_encoded_label = F.one_hot(label, num_classes=5).squeeze(0)\n",
    "        #ids = self.sp.encode(text, out_type=int)\n",
    "        ids = torch.tensor(text, dtype=torch.long)\n",
    "\n",
    "        return ids, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4aefb-8a0c-4c66-8aa5-be43c564aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_ex = 10\n",
    "icl_dataset = ICLDataset(annos_x, annos_y, shot_ex)\n",
    "icl_loader = DataLoader(\n",
    "    icl_dataset,\n",
    "    batch_size=50,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = ReviewDataset(train_df, \"amazon_reviews.model\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=50,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataset = ReviewDataset(test_df, \"amazon_reviews.model\")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=50,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "max_len = 0\n",
    "for padded, lengths, labels in icl_loader:\n",
    "    batch_max = lengths.max().item()\n",
    "    max_len = max(max_len, batch_max)\n",
    "    \n",
    "for padded, lengths, labels in train_loader:\n",
    "    batch_max = lengths.max().item()\n",
    "    max_len = max(max_len, batch_max)\n",
    "\n",
    "\n",
    "\n",
    "for padded, lengths, labels in test_loader:\n",
    "    batch_max = lengths.max().item()\n",
    "    max_len = max(max_len, batch_max)\n",
    "\n",
    "\n",
    "print(\"max_len\", max_len)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45292a79-0f1e-4ce9-b2af-97703dd946a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 79967237\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "\n",
    "model = DecoderOnlyTransformer(vocab_size+2, max_len = max_len)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 15\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {total_params}\")\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    track_loss = []\n",
    "    total_loss = 0\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        train_data_loader = tqdm(dataloader)\n",
    "        \n",
    "        for padded, lengths, labels in train_data_loader:\n",
    "            padded = padded.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(padded, lengths)\n",
    "\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            track_loss.append(loss.item())\n",
    "            avg = sum(track_loss[-10:])/10\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "            train_data_loader.set_postfix(loss=avg)\n",
    "            del outputs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        corr =0\n",
    "        ll = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for padded, lengths, labels in test_loader:\n",
    "                padded = padded.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(padded, lengths)\n",
    "\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                v = torch.argmax(outputs, dim =1)\n",
    "                delt = torch.sum(v==labels)\n",
    "                ll+= len(v)\n",
    "                corr += delt\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        print(\"Epoch\", i+1, \"track loss: \", total_loss / len(train_data_loader), \" val loss: \", total_val_loss / len(test_loader), \"val acc: \", corr / len_test )\n",
    "    return total_loss, track_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30911f4c-4b5c-4187-96cf-78c701394352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 308/308 [00:35<00:00,  8.70it/s, loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 track loss:  1.4716645417275367  val loss:  1.245039432079761 val acc:  tensor(0.4580, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 308/308 [00:35<00:00,  8.69it/s, loss=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 track loss:  1.2867689748089035  val loss:  1.1673883284841264 val acc:  tensor(0.4853, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 308/308 [00:35<00:00,  8.74it/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 track loss:  1.1828885115199275  val loss:  1.0738707641502478 val acc:  tensor(0.5355, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 308/308 [00:35<00:00,  8.73it/s, loss=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 track loss:  1.1285728078770947  val loss:  0.9870968190106478 val acc:  tensor(0.5833, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 308/308 [00:35<00:00,  8.62it/s, loss=0.958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 track loss:  1.0481786911750768  val loss:  0.9445680163123391 val acc:  tensor(0.6087, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 308/308 [00:35<00:00,  8.78it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 track loss:  0.9697362931517811  val loss:  0.8629723771826013 val acc:  tensor(0.6545, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 308/308 [00:35<00:00,  8.75it/s, loss=0.871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 track loss:  0.8766191284377853  val loss:  0.7315748159761553 val acc:  tensor(0.7228, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 308/308 [00:35<00:00,  8.63it/s, loss=0.826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 track loss:  0.7985039797502679  val loss:  0.6863829261296756 val acc:  tensor(0.7397, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 308/308 [00:35<00:00,  8.62it/s, loss=0.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 track loss:  0.714897434529546  val loss:  0.636178051496481 val acc:  tensor(0.7750, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 308/308 [00:35<00:00,  8.67it/s, loss=0.683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 track loss:  0.6331588693640449  val loss:  0.6883979702924753 val acc:  tensor(0.7480, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 308/308 [00:35<00:00,  8.74it/s, loss=0.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 track loss:  0.5938587326121021  val loss:  0.5822773641580111 val acc:  tensor(0.8184, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 308/308 [00:35<00:00,  8.59it/s, loss=0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 track loss:  0.5277755418187612  val loss:  0.6045065177725507 val acc:  tensor(0.8296, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 308/308 [00:35<00:00,  8.76it/s, loss=0.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 track loss:  0.4671752806994822  val loss:  0.6112581503081631 val acc:  tensor(0.8314, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 308/308 [00:35<00:00,  8.66it/s, loss=0.493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 track loss:  0.4291339156205778  val loss:  0.5750048967537942 val acc:  tensor(0.8485, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 308/308 [00:36<00:00,  8.55it/s, loss=0.449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 track loss:  0.41322324603989524  val loss:  0.6052446164094009 val acc:  tensor(0.8483, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "loss, tracked_loss = train(model, train_loader, optimizer, criterion, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b4e512b-60c0-492e-a0a6-883cc1b966f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e499dd5a-709b-45f4-aac4-bf99aeb12f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_val_loss = 0\n",
    "corr =0\n",
    "ll = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d653db2-81e3-4481-8c3d-96f6d6ad4483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:  tensor(0.8483, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def predict(text):\n",
    "    ids = torch.tensor(tok.encode(text, out_type=int)).unsqueeze(0)\n",
    "    lengths = torch.tensor([len(ids)])\n",
    "    ids = ids.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(ids, lengths)\n",
    "        ao = torch.argmax(out)\n",
    "    return ao\n",
    "\n",
    "#print(\"Prediction for <I love this book>\", predict(\"I love this book\"))\n",
    "\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "corr =0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for padded, lengths, labels in test_loader:\n",
    "        padded = padded.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(padded, lengths)\n",
    "\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        v = torch.argmax(outputs, dim =1)\n",
    "        delt = torch.sum(v==labels)\n",
    "        corr += delt\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        del outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"val acc: \", corr / len_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80e06dbb-0eef-4346-a95f-5c98e818c8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  tensor(0.9421, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_train_loss = 0\n",
    "corr =0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for padded, lengths, labels in train_loader:\n",
    "        padded = padded.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(padded, lengths)\n",
    "\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        v = torch.argmax(outputs, dim =1)\n",
    "        delt = torch.sum(v==labels)\n",
    "        corr += delt\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        del outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"train acc: \", corr / len_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92c941dc-dcb7-484e-b72b-cd475fbc7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLDataset(Dataset):\n",
    "    def __init__(self, shot_dict, y, shots):\n",
    "        self.ICL_shot = shot_dict[shots]\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ICL_shot)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.ICL_shot[idx]\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        \n",
    "        #label = self.df.loc[idx, \"overall\"].item()\n",
    "        #one_hot_encoded_label = F.one_hot(label, num_classes=5).squeeze(0)\n",
    "        #ids = self.sp.encode(text, out_type=int)\n",
    "        ids = torch.tensor(text, dtype=torch.long)\n",
    "\n",
    "        return ids, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591361c3-9e37-43d9-a453-4a4df78b171f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 5, 7, 10]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea6248a-e788-4df6-80f0-93a4fcdfa978",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ICLDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shot_ex \u001b[38;5;129;01min\u001b[39;00m n:\n\u001b[0;32m----> 2\u001b[0m     icl_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mICLDataset\u001b[49m(annos_x, annos_y, shot_ex)\n\u001b[1;32m      3\u001b[0m     icl_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      4\u001b[0m         icl_dataset,\n\u001b[1;32m      5\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      6\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m         collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ICLDataset' is not defined"
     ]
    }
   ],
   "source": [
    "for shot_ex in n:\n",
    "    icl_dataset = ICLDataset(annos_x, annos_y, shot_ex)\n",
    "    icl_loader = DataLoader(\n",
    "        icl_dataset,\n",
    "        batch_size=50,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    max_len = 0\n",
    "    for padded, lengths, labels in icl_loader:\n",
    "        batch_max = lengths.max().item()\n",
    "        max_len = max(max_len, batch_max)\n",
    "    \n",
    "    print(\"max_len\", max_len)\n",
    "    \n",
    "    \n",
    "    ll = 0\n",
    "    corr = 0\n",
    "    total_icl_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for padded, lengths, labels in icl_loader:\n",
    "            lengths = lengths.to(device)\n",
    "            labels =  labels.to(device)\n",
    "            padded = padded.to(device)\n",
    "    \n",
    "            outputs = model(padded, lengths)\n",
    "            v = torch.argmax(outputs, dim = 1)\n",
    "            corr += sum(v==labels)\n",
    "            ll += len(v)\n",
    "            print(max(v))\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_icl_loss += loss.item()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    avg_icl = total_icl_loss / len(icl_loader)\n",
    "    icl_acc = corr / ll\n",
    "    print(shot_ex, avg_icl, icl_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72126f61-3a12-41f1-a9f9-dd1e3e8157a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 553\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for padded, lengths, labels in icl_loader:\n",
    "    batch_max = lengths.max().item()\n",
    "    max_len = max(max_len, batch_max)\n",
    "\n",
    "print(\"max_len\", max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "308d0d52-f07f-4a69-b450-a7c8a94f07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLDataset(Dataset):\n",
    "    def __init__(self, shot_dict, y, shots):\n",
    "        self.ICL_shot = shot_dict[shots]\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ICL_shot)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.ICL_shot[idx]\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        \n",
    "        #label = self.df.loc[idx, \"overall\"].item()\n",
    "        #one_hot_encoded_label = F.one_hot(label, num_classes=5).squeeze(0)\n",
    "        #ids = self.sp.encode(text, out_type=int)\n",
    "        ids = torch.tensor(text, dtype=torch.long)\n",
    "\n",
    "        return ids, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d04e343-ffbb-499c-897f-990b996fe132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 5, 7, 10]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
