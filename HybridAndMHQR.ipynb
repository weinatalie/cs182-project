{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5f6a02-1018-4e50-8198-63e3a414e3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Hybrid with Transformer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/3000 [00:00<02:53, 17.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss 5.7731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 506/3000 [00:12<01:05, 38.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Loss 0.0315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 1005/3000 [00:26<00:53, 37.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Loss 0.0314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1505/3000 [00:39<00:38, 39.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500: Loss 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2005/3000 [00:51<00:22, 43.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000: Loss 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 2510/3000 [01:02<00:10, 44.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500: Loss 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:12<00:00, 41.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 1.0000\n",
      "\n",
      "--- Running Hybrid with Mamba ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/3000 [00:00<15:11,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss 6.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 501/3000 [02:18<12:43,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Loss 0.5328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1001/3000 [04:39<08:03,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Loss 0.5231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1501/3000 [06:59<07:52,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500: Loss 0.5178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2001/3000 [09:29<04:46,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000: Loss 0.5096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 2501/3000 [11:34<02:01,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500: Loss 0.5096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [13:45<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.1234\n",
      "\n",
      "--- Running Hybrid with LSTM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/3000 [00:00<00:52, 56.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss 6.2252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 515/3000 [00:04<00:20, 119.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Loss 1.2391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1020/3000 [00:08<00:16, 117.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Loss 1.1745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1523/3000 [00:12<00:12, 117.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500: Loss 1.1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2014/3000 [00:17<00:08, 119.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000: Loss 0.7013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 2524/3000 [00:21<00:04, 118.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500: Loss 0.6205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:25<00:00, 118.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.1244\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VOCAB_SIZE = 512       \n",
    "D_MODEL = 128          \n",
    "N_LAYERS = 4\n",
    "DROPOUT = 0.1\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_STEPS = 3000     # Slightly longer training for the harder task\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA GENERATORS\n",
    "# ==========================================\n",
    "\n",
    "class SyntheticDataset(IterableDataset):\n",
    "    def __init__(self, batch_size, seq_len):\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def generate_batch(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield self.generate_batch()\n",
    "\n",
    "class HybridArithmeticDataset(SyntheticDataset):\n",
    "    \"\"\"\n",
    "    NOVEL TASK: Hybrid Recall + Arithmetic\n",
    "    Context: 'a=3', 'b=5', 'c=2' ...\n",
    "    Query: 'a + 1 = ?' -> Target: '4'\n",
    "    \n",
    "    Why this is novel: \n",
    "    It forces the model to retrieve a specific symbol ('3') associated with a key ('a'),\n",
    "    move it to working memory, and then perform an algorithmic operation (+1) on it.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, seq_len, vocab_size=VOCAB_SIZE, num_vars=8):\n",
    "        super().__init__(batch_size, seq_len)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_vars = num_vars\n",
    "\n",
    "    def generate_batch(self):\n",
    "        input_ids = torch.zeros((self.batch_size, self.seq_len), dtype=torch.long)\n",
    "        targets = torch.zeros((self.batch_size, self.seq_len), dtype=torch.long)\n",
    "        \n",
    "        # Token Mapping:\n",
    "        # 0: Pad\n",
    "        # 1: '='\n",
    "        # 2: '+'\n",
    "        # 3: '?' (The \"Transform/Solve\" token)\n",
    "        # 10-19: Variables ('a', 'b', etc.)\n",
    "        # 20-50: Integer Values (0 to 30)\n",
    "        \n",
    "        VAR_START = 10\n",
    "        VAL_START = 20\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # 1. Create Variable Assignments\n",
    "            # Map random variables to random integer values (0-9)\n",
    "            vars_idx = list(range(self.num_vars))\n",
    "            random.shuffle(vars_idx)\n",
    "            \n",
    "            memory = {} # Truth Map: Token_ID -> Integer Value\n",
    "            seq = []\n",
    "            \n",
    "            for v_idx in vars_idx:\n",
    "                var_token = VAR_START + v_idx\n",
    "                val_int = random.randint(0, 9) \n",
    "                val_token = VAL_START + val_int\n",
    "                \n",
    "                memory[var_token] = val_int\n",
    "                \n",
    "                # Context: \"var = val\" -> [VAR, 1, VAL]\n",
    "                seq.extend([var_token, 1, val_token])\n",
    "            \n",
    "            # 2. Generate Query\n",
    "            # Pick a variable we defined\n",
    "            if not memory: # Safety check\n",
    "                input_ids[i] = torch.zeros(self.seq_len)\n",
    "                continue\n",
    "                \n",
    "            target_var_token = random.choice(list(memory.keys()))\n",
    "            stored_int = memory[target_var_token]\n",
    "            \n",
    "            # Operation: +1 (Keep simple for now to test pure mechanism)\n",
    "            op_token = 2 # '+'\n",
    "            operand = 1  # We always add 1 in this specific task version\n",
    "            \n",
    "            # Calculate Answer\n",
    "            answer_int = stored_int + operand\n",
    "            answer_token = VAL_START + answer_int\n",
    "            \n",
    "            # Query Sequence: \"var + 1 ?\" -> [VAR, 2, VAL_1, 3]\n",
    "            # Let's signify \"1\" as VAL_START + 1\n",
    "            one_token = VAL_START + 1\n",
    "            \n",
    "            seq.extend([target_var_token, op_token, one_token, 3]) \n",
    "            \n",
    "            # 3. Pad/Crop\n",
    "            if len(seq) > self.seq_len:\n",
    "                seq = seq[-(self.seq_len):] # Keep the end (query) if too long\n",
    "            else:\n",
    "                # Pad at the start (left padding) or end? \n",
    "                # For causal models, right padding is standard if using masks,\n",
    "                # but simple 0-padding works here.\n",
    "                seq = seq + [0]*(self.seq_len - len(seq))\n",
    "            \n",
    "            input_ids[i] = torch.tensor(seq)\n",
    "            \n",
    "            # 4. Target Generation\n",
    "            # Standard next-token prediction\n",
    "            t = torch.roll(input_ids[i], -1)\n",
    "            \n",
    "            # CRITICAL: We must ensure the token AFTER the '?' (3) is the ANSWER\n",
    "            try:\n",
    "                # Find the '?' token\n",
    "                query_pos = seq.index(3) \n",
    "                if query_pos < self.seq_len - 1:\n",
    "                    t[query_pos] = answer_token\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "            targets[i] = t\n",
    "            \n",
    "        return input_ids.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "class MQARDataset(SyntheticDataset):\n",
    "    \"\"\" Baseline Retrieval Task (for comparison) \"\"\"\n",
    "    def __init__(self, batch_size, seq_len, vocab_size=VOCAB_SIZE, num_pairs=8):\n",
    "        super().__init__(batch_size, seq_len)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_pairs = num_pairs\n",
    "\n",
    "    def generate_batch(self):\n",
    "        input_ids = torch.zeros((self.batch_size, self.seq_len), dtype=torch.long)\n",
    "        targets = torch.zeros((self.batch_size, self.seq_len), dtype=torch.long)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            keys = torch.randint(0, self.vocab_size, (self.num_pairs,))\n",
    "            values = torch.randint(0, self.vocab_size, (self.num_pairs,))\n",
    "            seq = torch.randint(0, self.vocab_size, (self.seq_len,))\n",
    "            \n",
    "            for j in range(self.num_pairs):\n",
    "                seq[2*j] = keys[j]\n",
    "                seq[2*j+1] = values[j]\n",
    "            \n",
    "            query_idx = random.randint(0, self.num_pairs - 1)\n",
    "            seq[-2] = keys[query_idx]\n",
    "            seq[-1] = values[query_idx]\n",
    "            \n",
    "            input_ids[i] = seq\n",
    "            targets[i] = torch.roll(seq, -1)\n",
    "            \n",
    "        return input_ids.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL ARCHITECTURES\n",
    "# ==========================================\n",
    "\n",
    "class BaselineTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(VOCAB_SIZE, D_MODEL)\n",
    "        self.pos_embed = nn.Embedding(512, D_MODEL)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=D_MODEL, nhead=4, batch_first=True)\n",
    "        self.transformer = nn.TransformerDecoder(self.decoder_layer, num_layers=N_LAYERS)\n",
    "        self.head = nn.Linear(D_MODEL, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.shape\n",
    "        pos = torch.arange(t, device=x.device).unsqueeze(0)\n",
    "        emb = self.embed(x) + self.pos_embed(pos)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(t).to(x.device)\n",
    "        out = self.transformer(emb, memory=emb, tgt_mask=mask)\n",
    "        return self.head(out)\n",
    "\n",
    "class BaselineLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(VOCAB_SIZE, D_MODEL)\n",
    "        self.lstm = nn.LSTM(input_size=D_MODEL, hidden_size=D_MODEL, num_layers=N_LAYERS, batch_first=True)\n",
    "        self.head = nn.Linear(D_MODEL, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)\n",
    "        out, _ = self.lstm(emb)\n",
    "        return self.head(out)\n",
    "\n",
    "# --- PURE PYTORCH MAMBA ---\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.d_inner = int(expand * d_model)\n",
    "        self.dt_rank = math.ceil(d_model / 16)\n",
    "        \n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner, out_channels=self.d_inner,\n",
    "            bias=True, kernel_size=d_conv, groups=self.d_inner, padding=d_conv - 1,\n",
    "        )\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + d_state * 2, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "        \n",
    "        A = torch.arange(1, d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def parallel_scan(self, u, delta, A, B, C, D):\n",
    "        batch, seq_len, d_inner = u.shape\n",
    "        d_state = A.shape[1]\n",
    "        deltaA = torch.exp(torch.einsum('b l d, d n -> b l d n', delta, A))\n",
    "        deltaB_u = torch.einsum('b l d, b l n, b l d -> b l d n', delta, B, u)\n",
    "        \n",
    "        x = torch.zeros((batch, d_inner, d_state), device=u.device)\n",
    "        ys = []\n",
    "        for t in range(seq_len):\n",
    "            x = deltaA[:, t] * x + deltaB_u[:, t]\n",
    "            y = torch.einsum('b d n, b l n -> b d', x, C[:, t].unsqueeze(1))\n",
    "            ys.append(y)\n",
    "        y = torch.stack(ys, dim=1)\n",
    "        y = y + u * D\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, _ = x.shape\n",
    "        xz = self.in_proj(x)\n",
    "        x_in, z = xz.chunk(2, dim=-1)\n",
    "        x_in = x_in.transpose(1, 2)\n",
    "        x_conv = self.conv1d(x_in)[:, :, :seq_len]\n",
    "        x_conv = self.act(x_conv).transpose(1, 2)\n",
    "        x_dbl = self.x_proj(x_conv)\n",
    "        delta, B, C = torch.split(x_dbl, [self.dt_rank, 16, 16], dim=-1)\n",
    "        delta = F.softplus(self.dt_proj(delta))\n",
    "        A = -torch.exp(self.A_log)\n",
    "        y = self.parallel_scan(x_conv, delta, A, B, C, self.D)\n",
    "        return self.out_proj(y * self.act(z))\n",
    "\n",
    "class MambaWrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(VOCAB_SIZE, D_MODEL)\n",
    "        self.layers = nn.ModuleList([\n",
    "            MambaBlock(d_model=D_MODEL, d_state=16, expand=2)\n",
    "            for _ in range(N_LAYERS) \n",
    "        ])\n",
    "        self.norm_f = nn.LayerNorm(D_MODEL)\n",
    "        self.head = nn.Linear(D_MODEL, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        x = self.norm_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# ==========================================\n",
    "# 3. RUNNER\n",
    "# ==========================================\n",
    "\n",
    "def get_accuracy(logits, targets):\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    correct = (preds == targets).float()\n",
    "    return correct.mean().item()\n",
    "\n",
    "def run_experiment(model_type, task_name):\n",
    "    print(f\"\\n--- Running {task_name} with {model_type} ---\")\n",
    "    \n",
    "    # Dataset Selection\n",
    "    if task_name == \"MQAR\":\n",
    "        dataset = MQARDataset(BATCH_SIZE, seq_len=64) \n",
    "    elif task_name == \"Hybrid\":\n",
    "        dataset = HybridArithmeticDataset(BATCH_SIZE, seq_len=64)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Task\")\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=None)\n",
    "    \n",
    "    # Model Selection\n",
    "    if model_type == \"Transformer\":\n",
    "        model = BaselineTransformer().to(DEVICE)\n",
    "    elif model_type == \"LSTM\":\n",
    "        model = BaselineLSTM().to(DEVICE)\n",
    "    elif model_type == \"Mamba\":\n",
    "        model = MambaWrapper().to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    iterator = iter(loader)\n",
    "    \n",
    "    for step in tqdm(range(TRAIN_STEPS)):\n",
    "        inputs, targets = next(iterator)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits.view(-1, VOCAB_SIZE), targets.view(-1))\n",
    "        \n",
    "        # FIX: Gradient Clipping for Mamba Stability\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print(f\"Step {step}: Loss {loss.item():.4f}\")\n",
    "\n",
    "    # Final Eval (Last Token)\n",
    "    model.eval()\n",
    "    eval_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(50):\n",
    "            inputs, targets = next(iterator)\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # For Hybrid, we want to check the token AFTER the '?'\n",
    "            # In our dataset generation, we ensure '?' is at a specific spot or end\n",
    "            # We will just check the last predicted token against the last target token\n",
    "            last_pred = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "            last_target = targets[:, -1]\n",
    "            \n",
    "            eval_acc += (last_pred == last_target).float().mean().item()\n",
    "            \n",
    "    print(f\"Final Test Accuracy: {eval_acc / 50:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Run Baseline Retrieval (MQAR)\n",
    "    # run_experiment(\"Transformer\", \"MQAR\")\n",
    "    # run_experiment(\"Mamba\", \"MQAR\")\n",
    "    \n",
    "    # 2. Run Novel Hybrid Task\n",
    "    run_experiment(\"Transformer\", \"Hybrid\")\n",
    "    run_experiment(\"Mamba\", \"Hybrid\")\n",
    "    \n",
    "    # 3. LSTM (Negative Control)\n",
    "    run_experiment(\"LSTM\", \"Hybrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50562174-b5dc-47be-9f39-2ac81e006e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
