{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ce0b2-065b-4800-9ab8-ad9756ec8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "file = \"amazon_review.csv\"\n",
    "\n",
    "data = pd.read_csv(file).dropna(ignore_index=True)\n",
    "data['overall'] = data[\"overall\"] - 1\n",
    "\n",
    "vocab_size = 16000\n",
    "seq_len = 512\n",
    "pad_token = 16008\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='amazon_reviews.txt',\n",
    "    model_prefix='amazon_reviews',\n",
    "    vocab_size=vocab_size,\n",
    "    model_type='unigram',\n",
    "    character_coverage=1.0\n",
    ")\n",
    "\n",
    "tok = spm.SentencePieceProcessor(model_file='amazon_reviews.model')\n",
    "\n",
    "filter_ = 1\n",
    "\n",
    "ls  = [len(tok.encode(i, out_type=int)) for i in data[\"reviewText\"]]\n",
    "\n",
    "data[\"lengths\"] = ls\n",
    "\n",
    "data_trunc = data[data[\"lengths\"]<=80]\n",
    "\n",
    "v = data_trunc[\"overall\"].value_counts()\n",
    "\n",
    "balanced_data = (\n",
    "    data_trunc.groupby(\"overall\")\n",
    "      .sample(n=min(v), random_state=42)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "len(balanced_data)\n",
    "\n",
    "df_shuffled = balanced_data.sample(frac=filter_, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the shuffled DataFrame\n",
    "train_size = 0.8\n",
    "train_df = df_shuffled.sample(frac=train_size, random_state=42).reset_index(drop=True)\n",
    "test_df = df_shuffled.drop(train_df.index).reset_index(drop=True)\n",
    "\n",
    "print(\"len train df: \", len(train_df))\n",
    "print(\"len test df: \", len(test_df))\n",
    "\n",
    "# get one input output pair with special tokens for later concatenation with other pairs for one sequence under max sequence length\n",
    "def getEncodingOpen(df, i):\n",
    "    reviewtext = \"Review: \"+ df[\"reviewText\"].iloc[i]\n",
    "    rating = \"Rating: \" \n",
    "    #row = [16000] + tok.encode(reviewText, out_type=int) + [16001] + tok.encode([int(df[\"overall\"].iloc[i])], out_type = int)\n",
    "    row = [16000] + tok.encode(reviewtext, out_type = int) + [16002] + tok.encode(rating, out_type = int)\n",
    "    correct_output_rating = int(df[\"overall\"].iloc[i])\n",
    "    row = torch.LongTensor(row)\n",
    "    correct_output_rating = torch.LongTensor([correct_output_rating])\n",
    "    return row, correct_output_rating\n",
    "# get one input output pair with special tokens for later concatenation with other pairs for one sequence under max sequence length\n",
    "def getEncoding(df, i):\n",
    "    reviewtext = \"Review: \"+ df[\"reviewText\"].iloc[i]\n",
    "    rating = \"Rating: \" #+ str(int(df[\"overall\"].iloc[i]))\n",
    "    score = int(df[\"overall\"].iloc[i])\n",
    "    row = [16000] + tok.encode(reviewtext, out_type = int) + [16002] + tok.encode(rating, out_type = int) +[score+16003]+ [16001]\n",
    "    return row\n",
    "\n",
    "\n",
    "d = {16000: \"<BOS>\", 16001: \"<EOS>\",16002: \"<SEP>\",16003: \"<0>\",16004: \"<1>\",16005: \"<2>\",16006: \"<3>\",16007: \"<4>\", 16008: \"PAD\"}\n",
    "\n",
    "def decode_seq(seq):\n",
    "    outp = \"\"\n",
    "    sofar = []\n",
    "    for i in seq:\n",
    "        if(i<=15999):\n",
    "            sofar.append(i)\n",
    "        else:\n",
    "            outp += tok.decode(sofar)\n",
    "            outp += d[i]\n",
    "            sofar = []\n",
    "    outp += tok.decode(sofar)\n",
    "    return outp\n",
    "\n",
    "\n",
    "decode_seq(getEncoding(train_df, 2))\n",
    "\n",
    "train_df[\"overall\"].iloc[2]\n",
    "\n",
    "def getShiftSeq(df_t, max_seq=1024):\n",
    "    seqs_x = []\n",
    "    seqs_y = []\n",
    "    seqs = []\n",
    "    c = []\n",
    "    for i in range(len(df_t)):\n",
    "        row = getEncoding(df_t, i)\n",
    "        if len(c) + len(row) > max_seq +1:\n",
    "            seqs_x.append(c[:-1])\n",
    "            seqs_y.append(c[1:])\n",
    "            seqs.append(c)\n",
    "            c = []\n",
    "        c.extend(row)\n",
    "    return seqs_x, seqs_y, seqs\n",
    "\n",
    "train_seqs_x, train_seqs_y, train_seqs = getShiftSeq(train_df, max_seq=seq_len)\n",
    "test_seqs_x, test_seqs_y, test_seqs = getShiftSeq(test_df, max_seq=seq_len)\n",
    "\n",
    "len(train_seqs_x[0])\n",
    "\n",
    "len(train_seqs[0])\n",
    "\n",
    "decode_seq(train_seqs[0][-15:])\n",
    "\n",
    "train_seqs_x[0][-15:]\n",
    "\n",
    "train_seqs[0][-15:]\n",
    "\n",
    "class TokenDatasetB(Dataset):\n",
    "    def __init__(self, seqs_x, seqs_y):\n",
    "        self.seqs_x = seqs_x\n",
    "        self.seqs_y = seqs_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.seqs_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.seqs_x[idx]), torch.LongTensor(self.seqs_y[idx])\n",
    "\n",
    "# Padding collate function for variable length sequences\n",
    "def collate_fnB(batch):\n",
    "    seqs_x, seqs_y = zip(*batch)\n",
    "    lens = [len(s) for s in seqs_x]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_x = torch.zeros(len(seqs_x), max_len, dtype=torch.long) + pad_token\n",
    "    padded_y = torch.zeros(len(seqs_y), max_len, dtype=torch.long) + pad_token\n",
    "    \n",
    "    for i, (x, y) in enumerate(zip(seqs_x, seqs_y)):\n",
    "        padded_x[i, :len(x)] = x\n",
    "        padded_y[i, :len(y)] = y\n",
    "    \n",
    "    return padded_x, padded_y, torch.LongTensor(lens)\n",
    "\n",
    "dataset = TokenDatasetB(train_seqs_x, train_seqs_y)\n",
    "train_loader = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn=collate_fnB)\n",
    "\n",
    "for batch_idx, (x, y, lengths) in enumerate(train_loader):\n",
    "    print(\"bi\")\n",
    "    print(batch_idx)\n",
    "    print(\"x\")\n",
    "    print(x.shape)\n",
    "    xl = x[0].tolist()\n",
    "    print(xl)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(decode_seq(x[0].tolist()))\n",
    "    print()\n",
    "    print(decode_seq(x[1].tolist()))\n",
    "    print()\n",
    "    print(decode_seq(x[2].tolist()))\n",
    "    print()\n",
    "    print(decode_seq(x[3].tolist()))\n",
    "    print()\n",
    "    print(decode_seq(x[4].tolist()))\n",
    "    print()\n",
    "    print(\"y\")\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    print(\"lengths\")\n",
    "    print(lengths.shape)\n",
    "    break\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")\n",
    "    device = 'cpu'\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, causal_mask, padding_mask):\n",
    "        # Self-attention (GPT-style)\n",
    "        h = self.ln1(x)\n",
    "        attn_out, _ = self.attn(\n",
    "            h, h, h,\n",
    "            attn_mask=causal_mask,\n",
    "            key_padding_mask=padding_mask,\n",
    "            need_weights=False\n",
    "        )\n",
    "        x = x + attn_out\n",
    "\n",
    "        # Feedforward\n",
    "        h = self.ln2(x)\n",
    "        ff_out = self.mlp(h)\n",
    "        x = x + ff_out\n",
    "\n",
    "        return x\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len=512,\n",
    "                 embed_dim=1024, num_heads=4,\n",
    "                 num_layers=4, mlp_dim=2048, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GPTBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        #self.head.weight = self.embed.weight  # weight tying\n",
    "\n",
    "    #def causal_mask(self, T, device):\n",
    "    #    mask = torch.triu(torch.ones(T, T, device=device), 1)\n",
    "    #    return mask * float(\"-inf\")\n",
    "\n",
    "    def causal_mask(self, T, device): \n",
    "        mask = torch.triu(torch.ones(T, T, device=device), 1) \n",
    "        return mask.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        B, T = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        tok = self.embed(x)\n",
    "        pos = self.pos(torch.arange(T, device=device)[None, :])\n",
    "        h = tok + pos\n",
    "\n",
    "        causal = self.causal_mask(T, device)     # (T, T)\n",
    "        pad_mask = (x == 0)                      # (B, T)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, causal, pad_mask)\n",
    "\n",
    "        h = self.ln_final(h)\n",
    "        return self.head(h)                      # (B, T, V)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, epochs=10, lr=1e-4, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    track_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        loader = tqdm(train_loader)\n",
    "        \n",
    "        for x, y, lengths in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(x, lengths)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            track_loss.append(loss.item())\n",
    "            avg_loss = sum(track_loss[-10:]) / 10\n",
    "            loader.set_postfix(loss=avg_loss)\n",
    "            del logits\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = TokenDatasetB(train_seqs_x, train_seqs_y)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fnB)\n",
    "\n",
    "\n",
    "# Initialize model (set vocab_size to your tokenizer's vocab size + special tokens)\n",
    "vocab_size = 16000+9  # Adjust based on your tokenizer\n",
    "model = DecoderOnlyTransformer(vocab_size=vocab_size,  num_layers=12)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "# Train\n",
    "model = train_model(model, train_loader, epochs=20, lr=1e-4)\n",
    "\n",
    "\n",
    "k = [345, 23, 44, 99]\n",
    "kt = torch.tensor(k)\n",
    "\n",
    "seq_len\n",
    "\n",
    "anno, score = getEncodingOpen(test_df, random.randint(0,len(test_df)))\n",
    "\n",
    "annot = torch.tensor(anno)\n",
    "\n",
    "decode_seq(anno.tolist())\n",
    "\n",
    "annot\n",
    "\n",
    "score\n",
    "\n",
    "lens = len(anno)\n",
    "anno_len = torch.tensor([lens])\n",
    "\n",
    "# Pad sequences\n",
    "padded_x = torch.zeros(1, 512, dtype=torch.long) + pad_token\n",
    "padded_x[0,:lens] = torch.tensor(anno)\n",
    "\n",
    "padded_x = padded_x.to(device)\n",
    "anno_len = anno_len.to(device)\n",
    "\n",
    "padded_x[0].shape\n",
    "\n",
    "decode_seq(padded_x[0].tolist())\n",
    "\n",
    "out = model(padded_x, anno_len)\n",
    "\n",
    "out.shape\n",
    "\n",
    "tokenout = torch.argmax(out,dim=2)[0]\n",
    "\n",
    "score\n",
    "\n",
    "tokenout.shape\n",
    "\n",
    "decode_seq(tokenout.tolist())\n",
    "\n",
    "\n",
    "\n",
    "class ICLDataset(Dataset):\n",
    "    def __init__(self, df, shots, seq_len):\n",
    "        self.df = df\n",
    "        self.shots = shots\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def getpre(self, idx, l):\n",
    "        for _ in range(10):\n",
    "            ixs = []\n",
    "            xs = []\n",
    "            for j in range(self.shots):\n",
    "                sel = idx\n",
    "                while(sel==idx):\n",
    "                    sel = random.randint(0, self.__len__() -1)\n",
    "                xs.extend(getEncoding(self.df, sel))\n",
    "            if(len(xs) + l <= seq_len):\n",
    "                return torch.LongTensor(xs)\n",
    "        raise ValueError(f\"can't fit {self.shots} examples in context\")\n",
    "                \n",
    "            \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = getEncodingOpen(self.df, idx)\n",
    "        l = len(x)\n",
    "        pre = self.getpre(idx, l)\n",
    "        icl_x = torch.cat((pre,x))\n",
    "        return icl_x, y\n",
    "\n",
    "# Padding collate function for variable length sequences\n",
    "def collate_fn_icl(batch):\n",
    "    seqs_x, y = zip(*batch)\n",
    "    lens = [len(s) for s in seqs_x]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_x = torch.zeros(len(seqs_x), max_len, dtype=torch.long) + pad_token\n",
    "    \n",
    "    for i, x in enumerate(seqs_x):\n",
    "        padded_x[i, :len(x)] = x\n",
    "    \n",
    "    return padded_x, torch.LongTensor(y), torch.LongTensor(lens)\n",
    "\n",
    "\n",
    "\n",
    "icl_dataset = ICLDataset(train_df, 0, seq_len)\n",
    "icl_loader = DataLoader(icl_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_icl)\n",
    "\n",
    "for batch_idx, (x, y, lengths) in enumerate(icl_loader):\n",
    "    print(\"bi\")\n",
    "    print(batch_idx)\n",
    "    print(\"x\")\n",
    "    print(x.shape)\n",
    "    xl = x[0].tolist()\n",
    "    print(xl)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(decode_seq(xl))\n",
    "    print(\"y\")\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    print(\"lengths\")\n",
    "    print(lengths.shape)\n",
    "    break\n",
    "\n",
    "def israting(s):\n",
    "    bnk = [\"0\",\"1\",\"2\",\"3\",\"4\"]\n",
    "    return s in bnk\n",
    "def isnum(s):\n",
    "    bnk = [str(n) for n in range(0,20)]\n",
    "    return s in bnk\n",
    "\n",
    "def check_token_list(token_list):\n",
    "    isratings = 0\n",
    "    isnums = 0\n",
    "    for token in token_list:\n",
    "        isratings += 1 if israting(token) else 0\n",
    "        isnums += 1 if isnum(token) else 0\n",
    "    return isratings, isnums\n",
    "\n",
    "tok.decode([15999])\n",
    "\n",
    "tok.decode([])\n",
    "\n",
    "pred\n",
    "\n",
    "d\n",
    "\n",
    "def get_score(x):\n",
    "    x = x.tolist()\n",
    "    for i in range(len(x)-1,0, -1):\n",
    "        if 16003<= x[i] <=16007:\n",
    "            return x[i]\n",
    "    return 20000\n",
    "\n",
    "\n",
    "test_ex = torch.tensor([ 7421,   292,     4,     6,   945,    35,   161,    19,    31,   161,\n",
    "             6,    18,   161,    19,    10,    29,    26,   143,   204,     3,\n",
    "            28,   321,    14,   143,    75,   460, 13254,   292, 16000, 16008,\n",
    "         16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008,\n",
    "         16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008,\n",
    "         16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008,\n",
    "         16008])\n",
    "vv = get_score(test_ex)\n",
    "\n",
    "vv\n",
    "\n",
    "def getTokenScore(t):\n",
    "    if(t==20000):\n",
    "        return '-'\n",
    "    return d[t][1]\n",
    "\n",
    "icl_dataset = ICLDataset(train_df, 0, seq_len)\n",
    "icl_loader = DataLoader(icl_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_icl)\n",
    "\n",
    "tot = 0\n",
    "num = 0\n",
    "ratings = 0\n",
    "correct = 0\n",
    "for batch_idx, (x, y, lengths) in enumerate(icl_loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    pred_logits = model(x,lengths)\n",
    "    print(x.shape)\n",
    "    print(x[0])\n",
    "    for sequence in x:\n",
    "        print(decode_seq(sequence.tolist()))\n",
    "        print()\n",
    "    print(\"========================\")\n",
    "    pred_tokens = torch.argmax(pred_logits,dim=2)\n",
    "    pred_tokens = pred_tokens.to('cpu')\n",
    "    #pred_tokens = trim_tail(pred_tokens, pad_token)\n",
    "    #pred_last_token = pred_tokens[:,-1].tolist()\n",
    "    pred_last_token = [get_score(i) for i in pred_tokens]\n",
    "    for sequence in pred_tokens:\n",
    "        print(decode_seq(sequence.tolist()))\n",
    "        print()\n",
    "    pred_scores = []\n",
    "    print(pred_last_token)\n",
    "    for token in pred_last_token:\n",
    "        if(token<=15999):\n",
    "            pred_scores.append(tok.decode([token]))\n",
    "        else:\n",
    "            pred_scores.append(getTokenScore(token))\n",
    "\n",
    "    print('---')\n",
    "    print(pred_scores)\n",
    "    \n",
    "    ys = [str(ans) for ans in y.tolist()]\n",
    "    print(ys)\n",
    "    tot += len(y)\n",
    "    for t in range(len(y)):\n",
    "        correct += 1 if pred_scores[t] == ys[t] else 0\n",
    "    isratings, isnums = check_token_list(pred_scores)\n",
    "    num += isnums\n",
    "    ratings += isratings\n",
    "    torch.cuda.empty_cache()\n",
    "    break\n",
    "\n",
    "print(\"tot: \", tot)\n",
    "print(\"num: \", num)\n",
    "print(\"ratings: \", ratings)\n",
    "print(\"correct: \", correct)\n",
    "\n",
    "icl_dataset = ICLDataset(train_df, 1, seq_len)\n",
    "icl_loader = DataLoader(icl_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_icl)\n",
    "\n",
    "tot = 0\n",
    "num = 0\n",
    "ratings = 0\n",
    "correct = 0\n",
    "for batch_idx, (x, y, lengths) in enumerate(icl_loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    pred_logits = model(x,lengths)\n",
    "    #print(x.shape)\n",
    "    #print(x[0])\n",
    "    #for sequence in x:\n",
    "    #    print(decode_seq(sequence.tolist()))\n",
    "    #    print()\n",
    "    #print(\"========================\")\n",
    "    pred_tokens = torch.argmax(pred_logits,dim=2)\n",
    "    pred_tokens = pred_tokens.to('cpu')\n",
    "    #pred_tokens = trim_tail(pred_tokens, pad_token)\n",
    "    #pred_last_token = pred_tokens[:,-1].tolist()\n",
    "    pred_last_token = [get_score(i) for i in pred_tokens]\n",
    "    #for sequence in pred_tokens:\n",
    "    #    print(decode_seq(sequence.tolist()))\n",
    "    #    print()\n",
    "    pred_scores = []\n",
    "    #print(pred_last_token)\n",
    "    for token in pred_last_token:\n",
    "        if(token<=15999):\n",
    "            pred_scores.append(tok.decode([token]))\n",
    "        else:\n",
    "            pred_scores.append(getTokenScore(token))\n",
    "\n",
    "    #print('---')\n",
    "    #print(pred_scores)\n",
    "    \n",
    "    ys = [str(ans) for ans in y.tolist()]\n",
    "    #print(ys)\n",
    "    tot += len(y)\n",
    "    for t in range(len(y)):\n",
    "        correct += 1 if pred_scores[t] == ys[t] else 0\n",
    "    isratings, isnums = check_token_list(pred_scores)\n",
    "    num += isnums\n",
    "    ratings += isratings\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"tot: \", tot)\n",
    "print(\"num: \", num)\n",
    "print(\"ratings: \", ratings)\n",
    "print(\"correct: \", correct)\n",
    "print(\"acc: \", correct/num)\n",
    "\n",
    "5358/15387 #0\n",
    "\n",
    " 4778 / 15396\n",
    "\n",
    "def run_icl_experiment(model, test_df, tokenizer, shot_counts=[0, 1, 3, 5, 10], num_samples=500, device='cuda'):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"Running ICL Experiment on {num_samples} samples per shot count...\")\n",
    "    \n",
    "    # Pre-compute samples to ensure fairness (same targets for all shots?)\n",
    "    # For simplicity and to match the previous logic, we'll generate fresh samples for each shot count\n",
    "    # or we can try to keep targets consistent. Let's follow the notebook's pattern.\n",
    "    \n",
    "    for k in shot_counts:\n",
    "        print(f\"\\nEvaluating {k}-shot performance...\")\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # We need to construct the prompts manually since ICLDataset logic was in a different notebook\n",
    "        # and might rely on specific variables. Let's implement a self-contained loop.\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # 1. Pick a target\n",
    "            target_idx = random.randint(0, len(test_df)-1)\n",
    "            target_row = test_df.iloc[target_idx]\n",
    "            target_review = str(target_row['reviewText'])\n",
    "            target_label = int(target_row['overall'])\n",
    "            \n",
    "            # 2. Pick k shots (excluding target)\n",
    "            shots_df = test_df.drop(target_idx)\n",
    "            if k > 0:\n",
    "                shots = shots_df.sample(n=k)\n",
    "            else:\n",
    "                shots = pd.DataFrame()\n",
    "            \n",
    "            # 3. Construct Prompt\n",
    "            # Format: [Review 1] [Rating 1] ... [Review Target] [Rating: ]\n",
    "            prompt_ids = [16000] # Start token if needed, or just start concatenation\n",
    "            # Actually, let's stick to the format used in training/prediction\n",
    "            # The model expects: [16000] Review [16002] Rating: [Label] [16001]\n",
    "            \n",
    "            full_prompt_ids = []\n",
    "            \n",
    "            # Add shots\n",
    "            for _, shot_row in shots.iterrows():\n",
    "                shot_review = str(shot_row['reviewText'])\n",
    "                shot_label = int(shot_row['overall'])\n",
    "                \n",
    "                # Shot encoding: [16000] Review [16002] Rating: Label [16001]\n",
    "                # Note: We must ensure this matches exactly what the model learned.\n",
    "                # In getEncoding: [16000] + review + [16002] + \"Rating: \" + label + [16001]\n",
    "                \n",
    "                row_ids = [16000] + tokenizer.encode(shot_review, out_type=int) + \\\n",
    "                          [16002] + tokenizer.encode(\"Rating: \", out_type=int) + \\\n",
    "                          [shot_label + 16003] + [16001]\n",
    "                full_prompt_ids.extend(row_ids)\n",
    "            \n",
    "            # Add target (without label)\n",
    "            # Target encoding: [16000] Review [16002] Rating:\n",
    "            target_ids = [16000] + tokenizer.encode(target_review, out_type=int) + \\\n",
    "                         [16002] + tokenizer.encode(\"Rating: \", out_type=int)\n",
    "            \n",
    "            full_prompt_ids.extend(target_ids)\n",
    "            \n",
    "            # 4. Truncate if too long (simple truncation from left)\n",
    "            # Max seq len is 512. If prompt is longer, we lose the start.\n",
    "            # Ideally we keep the target and as many shots as possible.\n",
    "            if len(full_prompt_ids) > 1024: # giving some buffer, though model trained on 1024\n",
    "                 full_prompt_ids = full_prompt_ids[-1024:]\n",
    "            \n",
    "            # 5. Predict\n",
    "            x = torch.LongTensor([full_prompt_ids]).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(x)\n",
    "                last_logits = logits[0, -1, ðŸ™‚\n",
    "                pred_id = torch.argmax(last_logits).item()\n",
    "            \n",
    "            pred_rating = pred_id - 16003\n",
    "            \n",
    "            if pred_rating == target_label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        acc = correct / total\n",
    "        results[k] = acc\n",
    "        print(f\"{k}-shot Accuracy: {acc:.4f}\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Run the experiment\n",
    "if 'model' in locals():\n",
    "    icl_results = run_icl_experiment(model, test_df, tok, shot_counts=[0, 1, 3, 5, 8, 10], num_samples=200)\n",
    "    print(\"\\nFinal Results:\", icl_results)\n",
    "else:\n",
    "    print(\"Model not loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
