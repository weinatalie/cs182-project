{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6325cc0a-faad-415f-b768-e8ff3ae59d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: amazon_reviews.txt\n",
      "  input_format: \n",
      "  model_prefix: amazon_reviews\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: amazon_reviews.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 112032 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=15424622\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=95\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 112032 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=8665450\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 83355 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 112032\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 90740\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 90740 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35198 obj=10.0011 num_tokens=205491 num_tokens/piece=5.83814\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=30303 obj=7.84363 num_tokens=207362 num_tokens/piece=6.84295\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22723 obj=7.80598 num_tokens=215430 num_tokens/piece=9.4807\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22710 obj=7.79839 num_tokens=215870 num_tokens/piece=9.5055\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17599 obj=7.83138 num_tokens=227650 num_tokens/piece=12.9354\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17598 obj=7.82446 num_tokens=227655 num_tokens/piece=12.9364\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: amazon_reviews.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: amazon_reviews.vocab\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "file = \"amazon_review.csv\"\n",
    "\n",
    "data = pd.read_csv(file).dropna(ignore_index=True)\n",
    "data['overall'] = data[\"overall\"] - 1\n",
    "\n",
    "vocab_size = 16000\n",
    "seq_len = 512\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='amazon_reviews.txt',\n",
    "    model_prefix='amazon_reviews',\n",
    "    vocab_size=vocab_size,\n",
    "    model_type='unigram',\n",
    "    character_coverage=1.0\n",
    ")\n",
    "\n",
    "tok = spm.SentencePieceProcessor(model_file='amazon_reviews.model')\n",
    "\n",
    "filter_ = 1\n",
    "\n",
    "ls  = [len(tok.encode(i, out_type=int)) for i in data[\"reviewText\"]]\n",
    "\n",
    "data[\"lengths\"] = ls\n",
    "\n",
    "data_trunc = data[data[\"lengths\"]<=40]\n",
    "\n",
    "v = data_trunc[\"overall\"].value_counts()\n",
    "\n",
    "balanced_data = (\n",
    "    data_trunc.groupby(\"overall\")\n",
    "      .sample(n=min(v), random_state=42)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "len(balanced_data)\n",
    "\n",
    "df_shuffled = balanced_data.sample(frac=filter_, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the shuffled DataFrame\n",
    "train_size = 0.8\n",
    "train_df = df_shuffled.sample(frac=train_size, random_state=42).reset_index(drop=True)\n",
    "test_df = df_shuffled.drop(train_df.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe9bc00-5382-476c-8d6a-37d37c8d6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one input output pair with special tokens for later concatenation with other pairs for one sequence under max sequence length\n",
    "def getEncodingOpen(df, i):\n",
    "    reviewtext = \"Review: \"+ df[\"reviewText\"].iloc[i]\n",
    "    rating = \"\\n Rating: \" \n",
    "    #row = [16000] + tok.encode(reviewText, out_type=int) + [16001] + tok.encode([int(df[\"overall\"].iloc[i])], out_type = int)\n",
    "    row = [16000] + tok.encode(reviewtext, out_type = int) + [16002] + tok.encode(rating, out_type = int)\n",
    "    correct_output_rating = int(df[\"overall\"].iloc[i])\n",
    "    row = torch.LongTensor(row)\n",
    "    correct_output_rating = torch.LongTensor([correct_output_rating])\n",
    "    return row, correct_output_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac402251-1edf-4d2f-bbec-c97bfdae231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one input output pair with special tokens for later concatenation with other pairs for one sequence under max sequence length\n",
    "def getEncoding(df, i):\n",
    "    reviewtext = \"Review: \"+ df[\"reviewText\"].iloc[i]\n",
    "    rating = \"Rating: \" + str(int(df[\"overall\"].iloc[i]))\n",
    "    #row = [16000] + tok.encode(reviewText, out_type=int) + [16001] + tok.encode([int(df[\"overall\"].iloc[i])], out_type = int)\n",
    "    row = [16000] + tok.encode(reviewtext, out_type = int) + [16002] + tok.encode(rating, out_type = int) + [16001]\n",
    "    return row\n",
    "\n",
    "def getShiftSeq(df_t, max_seq=1024):\n",
    "    seqs_x = []\n",
    "    seqs_y = []\n",
    "    seqs = []\n",
    "    c = []\n",
    "    for i in range(len(df_t)):\n",
    "        row = getEncoding(train_df, i)\n",
    "        if len(c) + len(row) > max_seq +1:\n",
    "            seqs_x.append(c[:-1])\n",
    "            seqs_y.append(c[1:])\n",
    "            seqs.append(c)\n",
    "            c = []\n",
    "        c.extend(row)\n",
    "    return seqs_x, seqs_y, seqs\n",
    "\n",
    "train_seqs_x, train_seqs_y, train_seqs = getShiftSeq(train_df, max_seq=seq_len)\n",
    "test_seqs_x, test_seqs_y, test_seqs = getShiftSeq(test_df, max_seq=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39283dd5-fa89-40ad-8261-587122857b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDatasetB(Dataset):\n",
    "    def __init__(self, seqs_x, seqs_y):\n",
    "        self.seqs_x = seqs_x\n",
    "        self.seqs_y = seqs_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.seqs_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.seqs_x[idx]), torch.LongTensor(self.seqs_y[idx])\n",
    "\n",
    "# Padding collate function for variable length sequences\n",
    "def collate_fnB(batch):\n",
    "    seqs_x, seqs_y = zip(*batch)\n",
    "    lens = [len(s) for s in seqs_x]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_x = torch.zeros(len(seqs_x), max_len, dtype=torch.long)\n",
    "    padded_y = torch.zeros(len(seqs_y), max_len, dtype=torch.long)\n",
    "    \n",
    "    for i, (x, y) in enumerate(zip(seqs_x, seqs_y)):\n",
    "        padded_x[i, :len(x)] = x\n",
    "        padded_y[i, :len(y)] = y\n",
    "    \n",
    "    return padded_x, padded_y, torch.LongTensor(lens)\n",
    "\n",
    "dataset = TokenDatasetB(train_seqs_x, train_seqs_y)\n",
    "train_loader = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn=collate_fnB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d463056-66fe-4da7-9128-e3e7f7bb7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi\n",
      "0\n",
      "x\n",
      "tensor([[16000,  7421,   292,  ..., 13254,   292,   303],\n",
      "        [16000,  7421,   292,  ...,     0,     0,     0],\n",
      "        [16000,  7421,   292,  ...,     0,     0,     0],\n",
      "        [16000,  7421,   292,  ...,   292,   468,     0],\n",
      "        [16000,  7421,   292,  ...,     0,     0,     0]])\n",
      "torch.Size([5, 510])\n",
      "y\n",
      "tensor([[ 7421,   292,  3765,  ...,   292,   303, 16001],\n",
      "        [ 7421,   292,    45,  ...,     0,     0,     0],\n",
      "        [ 7421,   292,   139,  ...,     0,     0,     0],\n",
      "        [ 7421,   292,   272,  ...,   468, 16001,     0],\n",
      "        [ 7421,   292,   315,  ...,     0,     0,     0]])\n",
      "torch.Size([5, 510])\n",
      "lengths\n",
      "torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([510, 497, 483, 509, 497])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch_idx, (x, y, lengths) in enumerate(train_loader):\n",
    "    print(\"bi\")\n",
    "    print(batch_idx)\n",
    "    print(\"x\")\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(\"y\")\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    print(\"lengths\")\n",
    "    print(lengths.shape)\n",
    "    break\n",
    "\n",
    "lengths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c553b33d-3abc-4f4e-96ba-c6e56fbf37bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "CUDA device count: 1\n",
      "Current CUDA device name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e18ed8-d2cc-4928-8ac3-3bdf65780839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, causal_mask, padding_mask):\n",
    "        # Self-attention (GPT-style)\n",
    "        h = self.ln1(x)\n",
    "        attn_out, _ = self.attn(\n",
    "            h, h, h,\n",
    "            attn_mask=causal_mask,\n",
    "            key_padding_mask=padding_mask,\n",
    "            need_weights=False\n",
    "        )\n",
    "        x = x + attn_out\n",
    "\n",
    "        # Feedforward\n",
    "        h = self.ln2(x)\n",
    "        ff_out = self.mlp(h)\n",
    "        x = x + ff_out\n",
    "\n",
    "        return x\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len=512,\n",
    "                 embed_dim=512, num_heads=4,\n",
    "                 num_layers=4, mlp_dim=1024, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GPTBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        self.head.weight = self.embed.weight  # weight tying\n",
    "\n",
    "    def causal_mask(self, T, device):\n",
    "        mask = torch.triu(torch.ones(T, T, device=device), 1)\n",
    "        return mask.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        B, T = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        tok = self.embed(x)\n",
    "        pos = self.pos(torch.arange(T, device=device)[None, :])\n",
    "        h = tok + pos\n",
    "\n",
    "        causal = self.causal_mask(T, device)     # (T, T)\n",
    "        pad_mask = (x == 0)                      # (B, T)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, causal, pad_mask)\n",
    "\n",
    "        h = self.ln_final(h)\n",
    "        return self.head(h)                      # (B, T, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28f38d8-8731-4171-8795-44da6aca6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, epochs=10, lr=1e-4, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    track_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        loader = tqdm(train_loader)\n",
    "        \n",
    "        for x, y, lengths in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(x, lengths)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            track_loss.append(loss.item())\n",
    "            avg_loss = sum(track_loss[-10:]) / 10\n",
    "            loader.set_postfix(loss=avg_loss)\n",
    "            del logits\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b60908-11f5-444c-93ae-fd8c2568c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenDatasetB(train_seqs_x, train_seqs_y)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fnB)\n",
    "\n",
    "\n",
    "# Initialize model (set vocab_size to your tokenizer's vocab size + special tokens)\n",
    "vocab_size = 16000+3  # Adjust based on your tokenizer\n",
    "model = DecoderOnlyTransformer(vocab_size=vocab_size,  num_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a8ea1a7-ac2e-4361-acea-13086fd9662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 33690112\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47aff4d4-b6ea-49ed-83f2-6e2d656301d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/273 [00:00<?, ?it/s]/home/rohan/Desktop/berkeley/cs182/cs182/lib/python3.10/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.57it/s, loss=11.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Average Loss: 24.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.53it/s, loss=7.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Average Loss: 9.0888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 273/273 [00:14<00:00, 18.72it/s, loss=6.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Average Loss: 6.7887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.70it/s, loss=5.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Average Loss: 5.7821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.69it/s, loss=5.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Average Loss: 5.2798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.63it/s, loss=4.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Average Loss: 4.9713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.66it/s, loss=4.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Average Loss: 4.7505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.67it/s, loss=4.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Average Loss: 4.5910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.52it/s, loss=4.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Average Loss: 4.4686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.67it/s, loss=4.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Average Loss: 4.3601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.57it/s, loss=4.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Average Loss: 4.2730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.76it/s, loss=4.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Average Loss: 4.1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 273/273 [00:14<00:00, 18.48it/s, loss=4.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Average Loss: 4.1324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.50it/s, loss=4.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Average Loss: 4.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.62it/s, loss=3.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Average Loss: 4.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.64it/s, loss=3.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Average Loss: 3.9539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.63it/s, loss=3.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Average Loss: 3.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.64it/s, loss=3.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Average Loss: 3.8452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.49it/s, loss=3.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Average Loss: 3.7915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 273/273 [00:14<00:00, 18.40it/s, loss=3.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Average Loss: 3.7394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = train_model(model, train_loader, epochs=20, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd87460a-0f31-4f0c-800d-ff8ffb727abb",
   "metadata": {},
   "source": [
    "# get one input output pair with special tokens for later concatenation with other pairs for one sequence under max sequence length\n",
    "def getEncoding(df, i):\n",
    "    reviewtext = \"Review: \"+ df[\"reviewText\"].iloc[i]\n",
    "    rating = \"\\n Rating: \" + str(int(df[\"overall\"].iloc[i]))\n",
    "    #row = [16000] + tok.encode(reviewText, out_type=int) + [16001] + tok.encode([int(df[\"overall\"].iloc[i])], out_type = int)\n",
    "    row = [16000] + tok.encode(reviewtext, out_type = int) + tok.encode(rating, out_type = int) + [16001]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ecf304-f0a4-431d-ad00-2c46047ec85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one input output pair with special tokens for later concatenation with other pairs for one sequence under max sequence length\n",
    "def getEncodingOpen(df, i):\n",
    "    reviewtext = \"Review: \"+ df[\"reviewText\"].iloc[i]\n",
    "    rating = \"\\n Rating: \" \n",
    "    #row = [16000] + tok.encode(reviewText, out_type=int) + [16001] + tok.encode([int(df[\"overall\"].iloc[i])], out_type = int)\n",
    "    row = [16000] + tok.encode(reviewtext, out_type = int) + tok.encode(rating, out_type = int)\n",
    "    correct_output_rating = int(df[\"overall\"].iloc[i])\n",
    "    row = torch.LongTensor(row)\n",
    "    correct_output_rating = torch.LongTensor([correct_output_rating])\n",
    "    return row, correct_output_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e21d8c29-ba36-41fe-9509-a107481bfbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [345, 23, 44, 99]\n",
    "kt = torch.tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcc225f7-a78e-4020-9977-d1fb388eb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno, score = getEncodingOpen(test_df, random.randint(0,len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbb9608b-ed31-42d0-a4cd-b94fdbe089c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19232/4078374235.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  annot = torch.tensor(anno)\n"
     ]
    }
   ],
   "source": [
    "annot = torch.tensor(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afdf5a43-2b53-4a28-b6d9-623da3bcd233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19232/1788454296.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_x[0,:lens] = torch.tensor(anno)\n"
     ]
    }
   ],
   "source": [
    "lens = len(anno)\n",
    "anno_len = torch.tensor([lens])\n",
    "\n",
    "# Pad sequences\n",
    "padded_x = torch.zeros(1, 98, dtype=torch.long)\n",
    "padded_x[0,:lens] = torch.tensor(anno)\n",
    "\n",
    "padded_x = padded_x.to(device)\n",
    "anno_len = anno_len.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c3453a3-b20d-46b8-874f-6afb88d26fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(padded_x, anno_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a76155-a067-4e39-817e-3312373d71ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 98, 16003])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f36c7b5-4122-4c48-8bef-970b56dc5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenout = torch.argmax(out,dim=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77190548-0dfa-4912-9462-ccd4e0f5e6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f9a6132-6236-43fc-8b02-22ea37a78894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7421,   292,     4,    82,    55,    11,     9,    46,    75,     6,\n",
       "           23,    27,     8,    11,     5,    74,     7,    17,    27,     8,\n",
       "          702,     7,    22,     8,    56,     3,     4,    27,     8,     8,\n",
       "           30,    30,     7,    56,     4,    27,    41,     3,   403,     8,\n",
       "          159,     7,    64,    21,   688,     7,     4,    88,     8,    30,\n",
       "            6,    30,     7,     8,   103,   141,    22,    53,   103,     3,\n",
       "        16002,   292,  3210,     3,     3,     3,     7,     3,    71,     3,\n",
       "            7,     3,     7,     3,     3,     3,     7,     3,     3,     3,\n",
       "            7,     9,   101,     3,     7,     3,   119,     7,   119,    57,\n",
       "            6,    31,     7,     3,     3,     3,     8,     3],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dada6bd-507c-4bf3-90d5-53036f14bd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db69ec8a-a371-4c5e-aec4-6cd6c47b1664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Review: I nice just for to be shoe, but have a for the little and of have a flat and they a than. I have a a size size and than I have wear.a aly and had them XL and I ordered a size, size and a long feet they too long.{: 0... and. at. and. and... and... and to years. and.ing anding up, this and... a.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tok.decode([min(15999,to) for to in tokenout.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0ba14-4687-46de-a45c-49d702ce45e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "056021ea-c2ee-4b3b-83a1-a39e04cb3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLDataset(Dataset):\n",
    "    def __init__(self, df, shots, seq_len):\n",
    "        self.df = df\n",
    "        self.shots = shots\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def getpre(self, idx, l):\n",
    "        for _ in range(10):\n",
    "            ixs = []\n",
    "            xs = []\n",
    "            for j in range(self.shots):\n",
    "                sel = idx\n",
    "                while(sel==idx):\n",
    "                    sel = random.randint(0, self.__len__() -1)\n",
    "                xs.extend(getEncoding(self.df, sel))\n",
    "            if(len(xs) + l <= seq_len):\n",
    "                return torch.LongTensor(xs)\n",
    "        raise ValueError(f\"can't fit {self.shots} examples in context\")\n",
    "                \n",
    "            \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = getEncodingOpen(self.df, idx)\n",
    "        l = len(x)\n",
    "        pre = self.getpre(idx, l)\n",
    "        icl_x = torch.cat((pre,x))\n",
    "        return icl_x, y\n",
    "\n",
    "# Padding collate function for variable length sequences\n",
    "def collate_fn_icl(batch):\n",
    "    seqs_x, y = zip(*batch)\n",
    "    lens = [len(s) for s in seqs_x]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_x = torch.zeros(len(seqs_x), max_len, dtype=torch.long)\n",
    "    \n",
    "    for i, x in enumerate(seqs_x):\n",
    "        padded_x[i, :len(x)] = x\n",
    "    \n",
    "    return padded_x, torch.LongTensor(y), torch.LongTensor(lens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a205d72-f62a-4c03-8f30-01030aebbc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_dataset = ICLDataset(test_df, 8, seq_len)\n",
    "icl_loader = DataLoader(icl_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_icl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cefcec9-c47a-4681-9065-2b98bd131630",
   "metadata": {},
   "source": [
    "for batch_idx, (x, y, lengths) in enumerate(icl_loader):\n",
    "    print(\"bi\")\n",
    "    print(batch_idx)\n",
    "    print(\"x\")\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(\"y\")\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    print(\"lengths\")\n",
    "    print(lengths.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "884c4939-b40f-4f44-a047-71c0b6e6da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def israting(s):\n",
    "    bnk = [\"0\",\"1\",\"2\",\"3\",\"4\"]\n",
    "    return s in bnk\n",
    "def isnum(s):\n",
    "    bnk = [str(n) for n in range(0,20)]\n",
    "    return s in bnk\n",
    "\n",
    "def check_token_list(token_list):\n",
    "    isratings = 0\n",
    "    isnums = 0\n",
    "    for token in token_list:\n",
    "        isratings += 1 if israting(token) else 0\n",
    "        isnums += 1 if isnum(token) else 0\n",
    "    return isratings, isnums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aaaa732b-6c79-42e0-93ee-f4bf7a3934ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode([15999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "755ae39c-767c-4cd6-b79a-d9f814cd9061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 338])\n",
      "tensor([16000,  7421,   292,    27,   162,   207,  1211,    50,    79,     6,\n",
      "           43,     5,    26,  3349,  2686,   144,   234,   282,    71,  1801,\n",
      "          747,  3108,    79,     3,   309,    79,     6,    25,  3147,    21,\n",
      "            7,    60,    63,    53,    78,     3,   543,   125,    33,   167,\n",
      "           53,    78,     3,  1193,   122,   161,    50,   543,   849,   381,\n",
      "         2017,   222,     7,    79,     3,    50,    63,  3196,     3,   259,\n",
      "           84,    53,   103,     3, 16002, 13254,   292,   468, 16001, 16000,\n",
      "         7421,   292,   398, 16002, 13254,   292,   259, 16001, 16000,  7421,\n",
      "          292,   139,    47,    34, 16002, 13254,   292,   259, 16001, 16000,\n",
      "         7421,   292,   181, 16002, 13254,   292,   303, 16001, 16000,  7421,\n",
      "          292,    87, 16002, 13254,   292,   303, 16001, 16000,  7421,   292,\n",
      "          706,     4,    43,    31,    75,    11,     5,    89,     3,   237,\n",
      "            5,   278,    99,   685,     9,  1424,   524,     7,    86,    43,\n",
      "          110,   185,    54,    17,    44,     3, 16002, 13254,   292,   186,\n",
      "        16001, 16000,  7421,   292,  1370,     9,    78,     6,    88,     8,\n",
      "           30,   331,    56,   242,    10,   108,   264,     3,  2424,    10,\n",
      "           29,   145,   350,    28,     3,  3554,   404,   271,     3, 16002,\n",
      "        13254,   292,   468, 16001, 16000,  7421,   292,   117,   191,    11,\n",
      "           65,     9,   395,     7,  2452,    18,   549, 11116,     3,    68,\n",
      "           31,   104,     3, 16002, 13254,   292,   303, 16001, 16000,  7421,\n",
      "          292,  1598,    10,    29,    26,    71,    60,     3,     4,   304,\n",
      "         1913,    41,     8,   543,  1686,    23,    31,    33,   375,    78,\n",
      "           28,    65,   460,     4,   554,    10,    29,  1238,    77,   226,\n",
      "            9,   633,     5,   453,    52, 12589,     3, 13254,   292,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       device='cuda:0')\n",
      "{ Review: have always purchased Dockers pants , like the fit....... Tried many pairs at major Dept . store , that sells them and all were too small. 38 waist was way too small. Purchased same pants 38 x 34 thru Amazon and . pants were approx. 3\" too long.{ Rating: 1{{ Review: received{ Rating: 3{{ Review: Very good!{ Rating: 3{{ Review: Good{ Rating: 4{{ Review: perfect{ Rating: 4{{ Review: Overall I like this shoe for the price. But the leather has started to fade already and look like an old pair of shoes.{ Rating: 2{{ Review: Way to small, ordered a size bigger than should've needed. Couldn't even try on. Typical costume fabric.{ Rating: 1{{ Review: big enough for me to read and hide my wrist tattoo. love this watch.{ Rating: 4{{ Review: Didn't fit at all. I usually u wear a 38DD but this was super small on me Also I couldn't figure out how to close the front-clasp. Rating: ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "{ Review: So the waist is 42 but the legs shrink instantly! It goes from 42 to like 30 waist. And my legs don't work like that{ Rating: 0{{ Review: Too small. I have other size 40's that fit with no problem. Looks like the foreign manufacturers have a different measuring ruler.{ Rating: 1{{ Review: Excellent pants, very satisfied with the seller.{ Rating: 4{{ Review: Order a large size of you have big thighs.{ Rating: 2{{ Review: Ordered the same size I wear in all of my jeans. (which are usually baggy). When I tried on the Levi's, I could not even zip them up, let alone buckle them. Had to send them back. A lot of money for something that doesn't fit. I do not know even a size bigger would fit.{ Rating: 0{{ Review: Too small.{ Rating: 0{{ Review: These are well made and the colors are nice, but not true to size my husband has several pairs of these exact pants in other colors and I bought his normal size in the same exact pants but a new color and they were way too small in the waist. We returned them and my husband now will only buy in store because of the inconvenience.{ Rating: 1{{ Review: At less than $12 each these were a good buy. They are well made, my main complaint in that the sleeves are too long. They are about 2 inches below the elbow. This may not be a problem if you are 6'4 but I am 5'10.{ Rating: 2{{ Review: Great Rating:\n",
      "\n",
      "{ Review: MY HUSBAND SAYS THEY'RE REALLY COMFORTABLE, GO GREAT FOR HANGING AROUND, AND DRESS-UP OCCASIONS TOO, LOTS OF COMPLEMENTS.{ Rating: 4{{ Review: Very Nice... Thank you.{ Rating: 3{{ Review: made boobs look a little cone shaped, but held them up DDD{ Rating: 2{{ Review: Pants rise way to high in the crotch. It is literally painful to walk up stairs.{ Rating: 1{{ Review: excelente{ Rating: 3{{ Review: To big in the shoulders. Fits and feels more like a light jacket than a shirt. There needs to be a lot more clarification on what the actual standard short sizes are for these types of products. A way to convert the listed military sizes to a civilian equivilent{ Rating: 2{{ Review: I used to buy these at the Hanes Bali store in the mall. The online ones run much smaller than the ones I used to get in the store. I would recommend you buy at least one size larger than usual.{ Rating: 2{{ Review: hides panty lines does not lift your butt however{ Rating: 2{{ Review: Wrong color. Correct color not available. Very disappointed Rating: ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "{ Review: I didn't like the color of these support hose. Also, the waist does not stay put. The whole panty slid down while I was wearing them. Not a comfortable feeling al all! I do not recommend these.{ Rating: 1{{ Review: I have enjoyed the fit and the style.{ Rating: 3{{ Review: Not as heavy weight as I would have liked. Fits a bit small for my taste.{ Rating: 2{{ Review: Fit well and seem pretty sturdy until both pairs I bought split in the crotch area just right of the zipper (not at the seam), just as others have stated. Disappointed.{ Rating: 1{{ Review: These Terramar silks are a disappointment. The silk is very rough/scratchy feeling. I bought two sets of the tops & bottoms but will return both.{ Rating: 0{{ Review: I do love them. But they are so expensive for a cheaply made shoe that they just aren't worth the price anymore. Immediate buyer's remorse.{ Rating: 0{{ Review: I bought each of my kids one of these watches for Christmas. The strap is easy to adjust, and the watch face is easy to read. I took off a star because both watches arrived with dead batteries.{ Rating: 3{{ Review: I ordered thinking it was a 8.5 M meaning Medium - not Men. I have to return it and won't have it for Christmas.{ Rating: 0{{ Review: great shoe Rating: ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "{ Review: Very, very small watch. Fit for kids, not adults, which isn't clear in the description. I gave it away to someone.{ Rating: 1{{ Review: Toddler actually means little kid size.{ Rating: 0{{ Review: Pants fit well but it's very hot. My legs sweat in them Very quickly. Not a summer pants at all{ Rating: 3{{ Review: Shoe inserts very thin and shaped wrong on the sides to fit my foot. Am ordering thicker insoles to try custom fitting. If that doesnt work, I am returning entire shoe order. Otherwise, the shoes seem well constructed and lightweight.{ Rating: 2{{ Review: Compared to the Sebago, these are slim fit shoes for those with the narrowest feet. These fit nothing like the canvas boat shoes from Sperry. I went with the Eddie Bauer boat shoe and have been incredibly happy.{ Rating: 1{{ Review: Nice shurt{ Rating: 3{{ Review: Good style but not ideal for warm days. Your feet will sweat in them and it'll become moist.{ Rating: 3{{ Review: Just too big{ Rating: 0{{ Review: doesn't fit Rating: ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "{ Review: The good stuff. Not the walmart kind with the button closure and thin material{ Rating: 3{{ Review: This is my second pair of keen water sandals. The first one lasted almost 10 years. Here's to another 10 with these.{ Rating: 4{{ Review: Excellent product!{ Rating: 4{{ Review: Zipper too short for use. Fabrication fair, found some miss placed stitches. Pants seem smaller size than stated. Hope Dickie realizes these issues.{ Rating: 1{{ Review: Foil/metallic finish peeled off in 2 days. Now my kid has regular teal chucks. Stick with regular chucks, fancy finish doesn't last!!!{ Rating: 1{{ Review: love em!{ Rating: 4{{ Review: The maker of these boots need to review their size specks. In the past I have been able to put boots on that were size 9d. I couldn't even get my foot past the turn in these boots that are supposed to be size 9EE. It could be that these boots were size marked wrong. I will never buy from this seller again.{ Rating: 0{{ Review: just too big{ Rating: 1{{ Review: It is impossible to buy Dickies over the internet without trying them on first. One style in \"loose fit\" may fit entirely differently in another style in the same fit. The product seems to be of good quality but the sizing is so far off as to not even be remotely helpful in making your purchase. Rating: ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "{ Review: Fits smaller than expected{ Rating: 2{{ Review: They run a bit large. I usually wear 12-13 and the 12s are pretty big. I think an 11 1/2 would fit me better.{ Rating: 4{{ Review: The material is too thin, it's not the same as I ordered 6 months ago, I will find a different t-shirt in the future.{ Rating: 0{{ Review: Just what I was looking for. Adequate pocket and a very nice \"medium weight\" T-shirt. I will be back for more!{ Rating: 4{{ Review: The waist is too big to keep them from sliding down. Rest was a perfect fit! Won't purchase again.{ Rating: 1{{ Review: Arrived on time material was too stiff for his liking{ Rating: 1{{ Review: I ordered these thinking I was getting a descent deal however, I was surprised when they arrived. Not only did they look cheap, I believe the sizing on the pair I received was mismarked. Sadly, I returned them.{ Rating: 1{{ Review: Love these panties no pinch and very comfy.{ Rating: 4{{ Review: This bra was too pointy for my chest -- empty fabric actually stuck out away from my body in a point at the tip of each cup. The bra definitely wasn't too big, because at the same time there was a little overflow at the top. What a weird fit. It's a pretty bra, but not the right shape for me. Rating: ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "{ Review: Fit is true to size and she loves them. She is fourteen and they are apparently all the rage. Not good for someone with a wide foot.{ Rating: 4{{ Review: Another pair I need to return. Back in the box.{ Rating: 0{{ Review: Wacoal bras are really soft and fit nicely, but this particular style does not have enough of a lining to provide adequate coverage. I did however, like the front clasp style. It's very flattering to wear but maybe only under a thick sweater.{ Rating: 2{{ Review: as decribed{ Rating: 4{{ Review: The waist of 38 was good but the length of 32 was too long.{ Rating: 1{{ Review: Not sure what problem is same size marked in shoe and on box as my Keds I have but would not fit. As always Amazon gave me no problem with the return!!{ Rating: 0{{ Review: Love these, my mom and her friend wear them, too.{ Rating: 4{{ Review: This bra has a very odd fit. The cups are placed very far apart and so, gives a very strange fit and feel.{ Rating: 1{{ Review: thanks Rating: ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "========================\n",
      "torch.Size([8, 338])\n",
      "Review: These been look this Dockers and, but a same is.......ed on years and the jeans3s,{.based I ares. they day not small.{x size a too long for{ds size. waists 34 a. they,{. not.{\" inseam small time{ Rating: 0{{ Review: These are Rating: 0{{ Review: These nice,{ Rating: 3{{ Review: The quality Rating: 3{{ Review: These. Rating: 3x{ Review: Great, ordered a one, my waist. It I waist. a to returns had not like this XL daughter of the.{ Rating: 4{{ Review: I too be{ but a little.. expected haves been.{'t even though to the{s. and{ Rating: 0{{ Review: These. for a. wear the the. foot..{ them is.{ Rating: 4{{ Review: These't like. least.{' wears a little.. not for a cute. my., ordered't wear out of long return to waist.line.{: 0 of,... up..... out to..... of...s withinging.ing.....,s.... the.,. of....sing. as..\".. to....ing.. them..,...... to.....ings.ing..... it. and.\". to.\n",
      "\n",
      "Review: These far fit is not and the waist are. and{ looks with the. be a inch.{ the foot are't even a a I Rating: 0{{ Review: I small{{ ordered a pants. yearss a the perfectly my arch with{ like a pants a of, been little colors tot{ Rating: 0{{ Review: The Excellent. not happy with a waist and{ Rating: 0{{ Review: These a little.. the wear a{.{ Rating: 0{{ Review: These a same size and ordered a the the the other are{Iwhich a wear,{ I ordered them the waist,s and and have be as close up. and and out, buckle.{ to be them..{ little of them. my something they't wear perfectly{ ordered not like, though little. than not.{ Rating: 0{{ Review: These small.{ Rating: 0{{ Review: The are a made in they size. not and not the as to be. other, a pairs of my pants same. the brands. they have them other size. the waist size size. they little shoes. I were too off small. the size.{ are them. they other has. not a again the. they the waist.{ Rating: 0{{ Review: These least than other. pair shoes too little quality again{ are a.. and other time is the they waist are a big.{ are not a{. the same.{ is be as a size. you have a have not have a\"{ Rating: 3{{ Review: These fit: 1\n",
      "\n",
      "Review: Ieds to THEYs in REALLY waist I., FOR.ING AROUND but AND.shirts.,'' blue nots. OFs,s{ Rating: 3{{ Review: These comfortable,{ you have{ Rating: 4{{ Review: The of, like little moreing. but the up... Rating: 3{{ Review: I are rise too be school the shoe.{' not., wear.. up{ Rating: 4{{ Review: I{ Rating: 3{{ Review: These big{ the size,{ great they like like a size weight. expected little.{ was to be a little of like. and as the you size part... a a shoes of the.{ little too be. size.. larger be little to.i. Rating: 0{{ Review: These expected to wear again for a size,,. the size.{ pants. are small smaller than a size are wanted to wear the the size.{ expected be. need. all one size. size expected size{ Rating: 0{{ Review: Theses,s. not as. money they wear, Rating: 0{{ Review: Very size is{ the and what.{ nice.: 0..{ ofs.s......s.ing.. the. I. of.... my..ing...\". and to.... out.. of... of.....s.....,s toing\"..., to.ed,.,..\n",
      "\n",
      "Review: The have't like the size, the are and.{, but size size not a up them{ material size hose shoe. just ordered a them.{ as size and to, the{' not buy them are{ Rating: 0{{ Review: These bought been the shoes. the shoes and{ Rating: 0{{ Review: Not as expected duty and expected have have been them{ perfectly little tight. a daughter.{ Rating: 0{{ Review: This as. fit to and and I pairs. have them. the same.. what away the same isI the all next seam), right a are a.{.{ Rating: 0{{ Review: These ares as. a size.{ only silk a happy and not fit.{ have these sizes and the same. looks. they be them shoes{ Rating: 0{{ Review: These have not them.{ they are a I. my little made.. I are a't even the material..{..t a-.s{ Rating: 0{{ Review: The have this other the other size size the are. my.{ pants. a to wear. not they first.. a to be.{ have them. little. the a and and a..{ Rating: 0{{ Review: The' them thinking. a little and and.. not be'{' a be them. the't even a. my present{ Rating: 0{{ Review: I,.: These. to..,... to. I. to to.....,.s..\n",
      "\n",
      "Review: I nice look pleased,, I is a have and a. it is's even. the right.{ will it. from be who{ Rating: 0{{ Review: I.' means small, and{ Rating: 0{{ Review: These are and. I wass a happy{{ husband,. a. nice.{ true little.. the the Rating: 3{{ Review: I is. happy, they and size my size of wear perfectly other,{ pair a.. wear to of.{ you' have. but ordered very thems. more{ the but size were to.. they.{ Rating: 0{{ Review: Thed a be right. I pants a fit and. a. a size. and.{ are. like them waist. shoe. a.{ ordered with a waist.. shoes. not a wearingyy with{ Rating: 0{{ Review: These shoe.. Rating: 3{{ Review: These quality of they as. a..{s. not. the. they wass be a the{ Rating: 3{{ Review: These what small. Rating: 4{{ Review: I't what.: 3ly........s with ofs. out.. to...s. to.. the. I. of....ss of a...... to....s...... size..... to...... to foring..,., them\"......\n",
      "\n",
      "Review: I size on. I sure sizes of the size.. size.. Rating: 0{{ Review: I is a size pair of the. shoes. I pants pair. about like..{ sizes not the pair. a shoes{ Rating: 0{{ Review: The fit was{ Rating: It{{ Review: I is small. a{{ is onely but the time miss placed of{ are to than. I.{ it' that. shoes.{ Rating: 1{{ Review: The size. cupsmetallic says on. the sizes.{ I feet. a size the are{ this this size. but..'t fit a{ Rating: 3{{ Review: These them about{ Rating: 0{{ Review: These size. the pants, to be is size. with{ 34 price, ordered been a to be them. the I a., like{ wear't wear though the foot. my size. the are,, a to wear a. for.{' not a' shoes are a. as size{' be. again the for and.{ Rating: 0{{ Review: I a small. Rating: 0{{ Review: These' a to be again pants the size. any to. the wash{ of. thes. perfectly. be... the pair. the waist size well{ size. like be a the quality. the shoes. a I as. expected be as close a. as. the it money.{: 0. is..... to.... to..........\n",
      "\n",
      "Review: I well than other. Rating: 4{{ Review: These are small size tight large I like wear size to fit same pair. comfortable,.{ have they inchs size have...{ Rating: 0{{ Review: I pants is a small and but iss a true next size expected' a foot.. but was have a size colors-shirts. the shoe.{ Rating: 0{{ Review: These a I ordered a for my{, foot. it little nice and hot them and.-shirts.{ ordered be a. a than{ Rating: 3{{ Review: I size. not small. bes. Amazon sliding.{ and a size..{'t fit..{ Rating: 3{{ Review: These as the. is a small and a foot. Rating: 3{{ Review: These expected a are thinking have as little.. the and have not to I are on{ as complaint not are nice. but was it waist chart the size of have them a.{, and have them.{ Rating: 0{{ Review: 0 them pants. way pants they comfortable and{ Rating: 0{{ Review: These was. a small and my legs.{. is a with of from a foot. the littleed the price. the each size{ quality. not't like small. but the all same size. is a little small. the size.{ else size. well{ ist. size much is and the even waist size. me.{: 0 is to... to.. with.. it....,ly.\n",
      "\n",
      "Review: These is a to be and the should them.{ is no as time it are not. the size.{ happy quality mys a little foot.{ Rating: 0{{ Review: The little size of bought to be.{ than a size was{ Rating: 0{{ Review: I of! a nice soft the. and but not shirt size. not as a. this little. be a..{ bought not, but it size...{ iss a comfortable flattering be a this a complaint my little and.{ Rating: 0{{ Review: These well. Rating: 0{{ Review: These color size thex not quality the same is the and a small.{ Rating: 0{{ Review: These as if I is not size.. the. it and is expected size, ordered a the be as.{ expected a. them. problem is this size them{ Rating: 0{{ Review: The the are and size, not.{.. and small{ Rating: 2{{ Review: I is is a little narrow to.{ pants are a placed small as. they much and a little happy. and they like{ Rating: 3{{ Review: These.: These.... are....ss. this..... iss.-.. that.-. of....ss. my.......s...s.. and,.,ed,.... to........ that.. to.. it....s you.\n",
      "\n",
      "['.', '1', '.', '.', '.', '.', '.', '.']\n",
      "[1, 3, 0, 1, 1, 0, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "num = 0\n",
    "ratings = 0\n",
    "correct = 0\n",
    "for batch_idx, (x, y, lengths) in enumerate(icl_loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    pred_logits = model(x,lengths)\n",
    "    print(x.shape)\n",
    "    print(x[0])\n",
    "    for sequence in x:\n",
    "        seqf = [min(int(to), 15999) for to in sequence]\n",
    "        print(tok.decode(seqf))\n",
    "        print()\n",
    "    print(\"========================\")\n",
    "    pred_tokens = torch.argmax(pred_logits,dim=2)\n",
    "    pred_tokens = pred_tokens.to('cpu')\n",
    "    pred_last_token = pred_tokens[:,-1].tolist()\n",
    "    print(pred_tokens.shape)\n",
    "    for sequence in pred_tokens:\n",
    "        seqf = [min(int(to), 15999) for to in sequence]\n",
    "        print(tok.decode(seqf))\n",
    "        print()\n",
    "    pred_scores = [tok.decode([token]) for token in pred_last_token]\n",
    "    print(pred_scores)\n",
    "    print(y.tolist())\n",
    "    tot += len(y)\n",
    "    for t in range(len(y)):\n",
    "        correct += 1 if pred_scores[t] == str(y[t]) else 0\n",
    "    isratings, isnums = check_token_list(pred_scores)\n",
    "    num += isnums\n",
    "    ratings += isratings\n",
    "    torch.cuda.empty_cache()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f502f9f4-9814-46d8-9f57-75d87a636bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot:  8\n",
      "num:  1\n",
      "ratings:  1\n",
      "correct:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"tot: \", tot)\n",
    "print(\"num: \", num)\n",
    "print(\"ratings: \", ratings)\n",
    "print(\"correct: \", correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68778e4c-f65d-426b-8099-ac08645be7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
