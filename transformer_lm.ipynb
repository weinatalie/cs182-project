{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6325cc0a-faad-415f-b768-e8ff3ae59d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: amazon_reviews.txt\n",
      "  input_format: \n",
      "  model_prefix: amazon_reviews\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: amazon_reviews.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 112032 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=15424622\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=95\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 112032 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=8665450\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 83355 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 112032\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 90740\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 90740 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35198 obj=10.0011 num_tokens=205491 num_tokens/piece=5.83814\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=30303 obj=7.84363 num_tokens=207362 num_tokens/piece=6.84295\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22723 obj=7.80598 num_tokens=215430 num_tokens/piece=9.4807\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22710 obj=7.79839 num_tokens=215870 num_tokens/piece=9.5055\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17599 obj=7.83138 num_tokens=227650 num_tokens/piece=12.9354\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17598 obj=7.82446 num_tokens=227655 num_tokens/piece=12.9364\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: amazon_reviews.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: amazon_reviews.vocab\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "file = \"amazon_review.csv\"\n",
    "\n",
    "data = pd.read_csv(file).dropna(ignore_index=True)\n",
    "data['overall'] = data[\"overall\"] - 1\n",
    "\n",
    "vocab_size = 16000\n",
    "seq_len = 512\n",
    "pad_token = 16008\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='amazon_reviews.txt',\n",
    "    model_prefix='amazon_reviews',\n",
    "    vocab_size=vocab_size,\n",
    "    model_type='unigram',\n",
    "    character_coverage=1.0\n",
    ")\n",
    "\n",
    "tok = spm.SentencePieceProcessor(model_file='amazon_reviews.model')\n",
    "\n",
    "filter_ = 1\n",
    "\n",
    "ls  = [len(tok.encode(i, out_type=int)) for i in data[\"reviewText\"]]\n",
    "\n",
    "data[\"lengths\"] = ls\n",
    "\n",
    "data_trunc = data[data[\"lengths\"]<=30]\n",
    "\n",
    "v = data_trunc[\"overall\"].value_counts()\n",
    "\n",
    "balanced_data = (\n",
    "    data_trunc.groupby(\"overall\")\n",
    "      .sample(n=min(v), random_state=42)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "len(balanced_data)\n",
    "\n",
    "df_shuffled = balanced_data.sample(frac=filter_, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the shuffled DataFrame\n",
    "train_size = 0.8\n",
    "train_df = df_shuffled.sample(frac=train_size, random_state=42).reset_index(drop=True)\n",
    "test_df = df_shuffled.drop(train_df.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bde877-51c4-416f-a49f-a446afd66bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train df:  9200\n",
      "len test df:  2300\n"
     ]
    }
   ],
   "source": [
    "print(\"len train df: \", len(train_df))\n",
    "print(\"len test df: \", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd715122-2863-46f9-b54f-337f957cb071",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe9bc00-5382-476c-8d6a-37d37c8d6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one input output pair with special tokens for later concatenation with other pairs for one sequence under max sequence length\n",
    "def getEncodingOpen(df, i):\n",
    "    reviewtext = \"Review: \"+ df[\"reviewText\"].iloc[i]\n",
    "    rating = \"Rating: \" \n",
    "    #row = [16000] + tok.encode(reviewText, out_type=int) + [16001] + tok.encode([int(df[\"overall\"].iloc[i])], out_type = int)\n",
    "    row = [16000] + tok.encode(reviewtext, out_type = int) + [16002] + tok.encode(rating, out_type = int)\n",
    "    correct_output_rating = int(df[\"overall\"].iloc[i])\n",
    "    row = torch.LongTensor(row)\n",
    "    correct_output_rating = torch.LongTensor([correct_output_rating])\n",
    "    return row, correct_output_rating\n",
    "# get one input output pair with special tokens for later concatenation with other pairs for one sequence under max sequence length\n",
    "def getEncoding(df, i):\n",
    "    reviewtext = \"Review: \"+ df[\"reviewText\"].iloc[i]\n",
    "    rating = \"Rating: \" #+ str(int(df[\"overall\"].iloc[i]))\n",
    "    score = int(df[\"overall\"].iloc[i])\n",
    "    row = [16000] + tok.encode(reviewtext, out_type = int) + [16002] + tok.encode(rating, out_type = int) +[score+16003]+ [16001]\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36d89abc-9f62-47ef-a2fa-9d3d372664a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {16000: \"<BOS>\", 16001: \"<EOS>\",16002: \"<SEP>\",16003: \"<0>\",16004: \"<1>\",16005: \"<2>\",16006: \"<3>\",16007: \"<4>\", 16008: \"PAD\"}\n",
    "\n",
    "def decode_seq(seq):\n",
    "    outp = \"\"\n",
    "    sofar = []\n",
    "    for i in seq:\n",
    "        if(i<=15999):\n",
    "            sofar.append(i)\n",
    "        else:\n",
    "            outp += tok.decode(sofar)\n",
    "            outp += d[i]\n",
    "            sofar = []\n",
    "    outp += tok.decode(sofar)\n",
    "    return outp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c4e1c26-7e3b-4e68-8cf0-30628b15dc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS>Review: The legs are to long. Ordered a 30\" inseam. A person with a 33\" inseam could wear these with no problem.<SEP>Rating:<2><EOS>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(getEncoding(train_df, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e261a40-3aab-46fa-a6dd-6adc45c8f8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"overall\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63e09b6f-98e0-41bc-8f7c-1a9f65f7122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac402251-1edf-4d2f-bbec-c97bfdae231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getShiftSeq(df_t, max_seq=1024):\n",
    "    seqs_x = []\n",
    "    seqs_y = []\n",
    "    seqs = []\n",
    "    c = []\n",
    "    for i in range(len(df_t)):\n",
    "        row = getEncoding(df_t, i)\n",
    "        if len(c) + len(row) > max_seq +1:\n",
    "            seqs_x.append(c[:-1])\n",
    "            seqs_y.append(c[1:])\n",
    "            seqs.append(c)\n",
    "            c = []\n",
    "        c.extend(row)\n",
    "    return seqs_x, seqs_y, seqs\n",
    "\n",
    "train_seqs_x, train_seqs_y, train_seqs = getShiftSeq(train_df, max_seq=seq_len)\n",
    "test_seqs_x, test_seqs_y, test_seqs = getShiftSeq(test_df, max_seq=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e53f0d4-f49d-446b-8487-139d77e6c14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seqs_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "081e32ee-e1bb-4647-95fb-7740cfbe50bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6f7c6b0-9afd-4c15-95cf-17d1bb41de8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good products, but they have become complete crap.<SEP>Rating:<0><EOS>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(train_seqs[0][-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26940807-a051-484c-91a4-8a17acb2ffce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[238, 47, 686, 6, 23, 22, 27, 1097, 2032, 2912, 3, 16002, 13254, 292, 16003]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seqs_x[0][-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06a05b5f-994c-4ccf-aedb-7469638b3902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 686, 6, 23, 22, 27, 1097, 2032, 2912, 3, 16002, 13254, 292, 16003, 16001]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seqs[0][-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39283dd5-fa89-40ad-8261-587122857b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDatasetB(Dataset):\n",
    "    def __init__(self, seqs_x, seqs_y):\n",
    "        self.seqs_x = seqs_x\n",
    "        self.seqs_y = seqs_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.seqs_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.seqs_x[idx]), torch.LongTensor(self.seqs_y[idx])\n",
    "\n",
    "# Padding collate function for variable length sequences\n",
    "def collate_fnB(batch):\n",
    "    seqs_x, seqs_y = zip(*batch)\n",
    "    lens = [len(s) for s in seqs_x]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_x = torch.zeros(len(seqs_x), max_len, dtype=torch.long) + pad_token\n",
    "    padded_y = torch.zeros(len(seqs_y), max_len, dtype=torch.long) + pad_token\n",
    "    \n",
    "    for i, (x, y) in enumerate(zip(seqs_x, seqs_y)):\n",
    "        padded_x[i, :len(x)] = x\n",
    "        padded_y[i, :len(y)] = y\n",
    "    \n",
    "    return padded_x, padded_y, torch.LongTensor(lens)\n",
    "\n",
    "dataset = TokenDatasetB(train_seqs_x, train_seqs_y)\n",
    "train_loader = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn=collate_fnB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d463056-66fe-4da7-9128-e3e7f7bb7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi\n",
      "0\n",
      "x\n",
      "torch.Size([5, 510])\n",
      "[16000, 7421, 292, 39, 971, 521, 5344, 52, 4871, 6, 4, 27, 127, 123, 24, 11, 177, 1508, 101, 7, 24, 19, 5, 157, 25, 19, 36, 5344, 52, 4871, 3, 16002, 13254, 292, 16004, 16001, 16000, 7421, 292, 1480, 36, 334, 57, 9, 432, 41, 7, 1111, 71, 60, 6, 628, 733, 186, 818, 233, 22, 63, 7069, 3, 16002, 13254, 292, 16004, 16001, 16000, 7421, 292, 815, 303, 215, 3, 39, 26, 352, 9, 46, 60, 177, 5, 2621, 3, 1225, 371, 151, 184, 3, 16002, 13254, 292, 16005, 16001, 16000, 7421, 292, 39, 44, 58, 46, 452, 3, 109, 13, 36, 18, 4496, 3, 1110, 327, 6, 244, 7, 634, 3, 16002, 13254, 292, 16004, 16001, 16000, 7421, 292, 96, 50, 63, 314, 92, 214, 63, 163, 11, 651, 16002, 13254, 292, 16007, 16001, 16000, 7421, 292, 4, 111, 10, 29, 223, 415, 8969, 432, 75, 276, 124, 908, 10, 131, 241, 2875, 3, 16002, 13254, 292, 16005, 16001, 16000, 7421, 292, 139, 82, 148, 3, 4, 43, 14, 34, 16002, 13254, 292, 16006, 16001, 16000, 7421, 292, 109, 8, 8, 2070, 415, 7, 645, 43, 5, 523, 1395, 3, 431, 46, 743, 1036, 7, 36, 5999, 3, 16002, 13254, 292, 16003, 16001, 16000, 7421, 292, 3015, 10, 29, 91, 165, 16002, 13254, 292, 16007, 16001, 16000, 7421, 292, 606, 156, 3, 8297, 268, 62, 30, 199, 56, 136, 439, 30, 35, 871, 19, 1499, 9, 166, 156, 3, 5791, 8, 30, 199, 13, 36, 191, 3, 16002, 13254, 292, 16005, 16001, 16000, 7421, 292, 3563, 16002, 13254, 292, 16006, 16001, 16000, 7421, 292, 346, 30, 36, 35, 2075, 3, 4, 1043, 25, 16, 200, 635, 7, 190, 36, 785, 14, 192, 2044, 3, 37, 2512, 30, 36, 35, 2075, 67, 88, 16002, 13254, 292, 16003, 16001, 16000, 7421, 292, 96, 50, 19, 36, 1971, 6, 25, 69, 27, 127, 82, 9, 227, 1922, 17, 73, 3, 1186, 15, 78, 34, 16002, 13254, 292, 16004, 16001, 16000, 7421, 292, 45, 809, 532, 9, 78, 3, 4, 88, 543, 15, 7, 22, 26, 43, 466, 15, 34, 16002, 13254, 292, 16003, 16001, 16000, 7421, 292, 4, 401, 22, 69, 27, 26, 3, 16002, 13254, 292, 16006, 16001, 16000, 7421, 292, 53, 244, 9, 209, 28, 16002, 13254, 292, 16004, 16001, 16000, 7421, 292, 616, 10, 29, 150, 346, 32, 721, 34, 1186, 15, 8, 74, 117, 16002, 13254, 292, 16007, 16001, 16000, 7421, 292, 1134, 28, 73, 6, 430, 35, 12872, 3, 550, 138, 165, 16002, 13254, 292, 16007, 16001, 16000, 7421, 292, 751, 33, 199, 56, 105, 3, 3318, 149, 13, 92, 4, 219, 21, 11, 3, 1492, 13, 8, 120, 797, 3, 16002, 13254, 292, 16005, 16001, 16000, 7421, 292, 2822, 403, 184, 11, 5, 961, 30, 16002, 13254, 292, 16005, 16001, 16000, 7421, 292, 336, 47, 6, 15075, 33, 36, 25, 47, 3, 16002, 13254, 292, 16006, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008]\n",
      "\n",
      "<BOS>Review: The description says Wrinkle-resistant, I have been wearing these for over ten years and these are the first that are not Wrinkle-resistant.<SEP>Rating:<1><EOS><BOS>Review: Did not hold up to normal wear and tear at all, maybe lasted 2 weeks before they were trashed.<SEP>Rating:<1><EOS><BOS>Review: Ordered 4 shirts. The fit seems to be all over the board. Some loose some tight.<SEP>Rating:<2><EOS><BOS>Review: The shoes will be returned. This is not my expectation. To narrow, hard and expensive.<SEP>Rating:<1><EOS><BOS>Review: These pants were exactly what we were looking for :)<SEP>Rating:<4><EOS><BOS>Review: I don't think converse follows normal shoe sizes...they're pretty oversized.<SEP>Rating:<2><EOS><BOS>Review: Very nice shirt. I like it!<SEP>Rating:<3><EOS><BOS>Review: This a a fake converse and nothing like the picture shown. Will be returning immediately and not reordering.<SEP>Rating:<0><EOS><BOS>Review: Won't buy again<SEP>Rating:<4><EOS><BOS>Review: Too large. Suggest going one size smaller than your regular size as chucks are known to run large. Half a size smaller is not enough.<SEP>Rating:<2><EOS><BOS>Review: alright<SEP>Rating:<3><EOS><BOS>Review: wrong size not as marked. I saw that in another review and did not pay it any attention. so beware size not as marked or ordered<SEP>Rating:<0><EOS><BOS>Review: These pants are not hemmed, that would have been nice to know ahead of time. Runs small!<SEP>Rating:<1><EOS><BOS>Review: They ran extremely to small. I ordered 38s and they fit like 32s!<SEP>Rating:<0><EOS><BOS>Review: I wish they would have fit.<SEP>Rating:<3><EOS><BOS>Review: too hard to put on<SEP>Rating:<1><EOS><BOS>Review: Can't go wrong with Chucks! Runs a little big<SEP>Rating:<4><EOS><BOS>Review: Arrived on time, everything as discribed. Would order again<SEP>Rating:<4><EOS><BOS>Review: Size was smaller than expected. Lightweight which is what I wanted them for. Material is a bit rough.<SEP>Rating:<2><EOS><BOS>Review: Kinda tight for the waste size<SEP>Rating:<2><EOS><BOS>Review: Fits good,delivery was not that good.<SEP>Rating:<3>PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "<BOS>Review: Fit large as expected, great for work (food service). Well constructed and functional<SEP>Rating:<3><EOS><BOS>Review: Tagged 48\" in waist and 30\" in inseam. They were actually 6\" too small. Most likely a factory error.<SEP>Rating:<0><EOS><BOS>Review: They do run a little big but they are pretty comfortable. I like the height and shape of the heel.<SEP>Rating:<2><EOS><BOS>Review: I will give this item a 5 star rating. I have used this type of bra for several years and will continue to do so.<SEP>Rating:<4><EOS><BOS>Review: Size 32 waist is actually size 30. Length of 32 is a actually a length of 34. -Sizing is way off.<SEP>Rating:<0><EOS><BOS>Review: This product is still very much like the just wearing a few days would like to<SEP>Rating:<3><EOS><BOS>Review: Size ran a little bit bigger too loose<SEP>Rating:<2><EOS><BOS>Review: Not enough support, quality not good,just didn't meet my expections<SEP>Rating:<1><EOS><BOS>Review: Price was great, condition perfect, size was perfect, shipped quick.<SEP>Rating:<4><EOS><BOS>Review: Falling apart after 2 monyh<SEP>Rating:<1><EOS><BOS>Review: Great quality however runs about a size small<SEP>Rating:<2><EOS><BOS>Review: Little baggy. Hopefully washing them will take care of that.<SEP>Rating:<2><EOS><BOS>Review: They aren't true to size . They are a full size smaller in the legs than my Jocky underwear !<SEP>Rating:<0><EOS><BOS>Review: doesn't fit. the iron inside hurts. I wore once and then threw away.<SEP>Rating:<0><EOS><BOS>Review: The material is somewhat durable and my niece loved it.<SEP>Rating:<3><EOS><BOS>Review: I liked the color and texture of the fabric was as expected<SEP>Rating:<3><EOS><BOS>Review: Returned these because they were too small and narrow for my feet. Liked the style, height of heel and look but not the fit. Too bad.<SEP>Rating:<2><EOS><BOS>Review: much smaller than expected..order a size or two up...<SEP>Rating:<1><EOS><BOS>Review: Fits ok but its a really dark brown much darker than the picture... almost black.<SEP>Rating:<2><EOS><BOS>Review: I definitely hate these thigh highs. Weird color, terrible quality.<SEP>Rating:<0><EOS><BOS>Review: very nice pants and wash beautifully<SEP>Rating:<3>\n",
      "\n",
      "<BOS>Review: The owner likes them, her only complaint is that they sometime can make her feet hot.<SEP>Rating:<3><EOS><BOS>Review: Watch out these pants run real small in the waist. Get a size larger than normal<SEP>Rating:<1><EOS><BOS>Review: Good quality, but runs a little large, but very comfortable. I would buy it again!<SEP>Rating:<3><EOS><BOS>Review: Wife hates these. But they are great for working in the garden. Not much more to say about overalls. Well made<SEP>Rating:<3><EOS><BOS>Review: great<SEP>Rating:<4><EOS><BOS>Review: I absolutely love them! True to size, came right on time, and are as pictured. They go with everything!<SEP>Rating:<4><EOS><BOS>Review: Size was good but matches can be lit by striking on these pants. Holy cow! I could use the fabric as sandpaper.<SEP>Rating:<2><EOS><BOS>Review: narrow for a WW width<SEP>Rating:<1><EOS><BOS>Review: These hose ran the first time I wore them; so not worth the money. They ran true to their size but the quality was lacking.<SEP>Rating:<0><EOS><BOS>Review: LUV THE SHOE BUT THE SIZE RUNS SMALL...RETURNED TO GET A LARGER SIZE<SEP>Rating:<2><EOS><BOS>Review: The shaft is to narrow and I am returning them.<SEP>Rating:<1><EOS><BOS>Review: Material lighter than I expected.<SEP>Rating:<2><EOS><BOS>Review: Converse are unisex. website made it confusing ended up two sizes too small for my wife now will need to send them back for exchange<SEP>Rating:<0><EOS><BOS>Review: Tried to switch from similar shoe by Timberland but Sperry's \"W\" was too narrow to fit me.<SEP>Rating:<1><EOS><BOS>Review: It was a good costume, kind of cheap looking but did the job.<SEP>Rating:<3><EOS><BOS>Review: Like all Dickey products like usual the size is not true don't fit right in the crotch<SEP>Rating:<2><EOS><BOS>Review: Waist was a little tight and the legs are way too wide. I have to pay $21 to have the legs altered<SEP>Rating:<2><EOS><BOS>Review: These underwear are pretty and the material is fine. That said, they do not stay in place on the backside. Not comfortable.<SEP>Rating:<1><EOS><BOS>Review: Had to return :( too small<SEP>Rating:<2>PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "<BOS>Review: I got this for my mother and it was to small for her. It would probably fit a small sized woman or a teenager alright.<SEP>Rating:<1><EOS><BOS>Review: These are classic Levi's jeans. Been wearing 517 Levi's most of my life, and I hope they never stop making them.<SEP>Rating:<4><EOS><BOS>Review: Was sent incorrect size. Material seemed nice but bust seemed to run small and cup size they sent was a size up.<SEP>Rating:<2><EOS><BOS>Review: Great buy for the money<SEP>Rating:<4><EOS><BOS>Review: Great work pants for my husband<SEP>Rating:<4><EOS><BOS>Review: Trash, wouldn't stay up.<SEP>Rating:<0><EOS><BOS>Review: The quality is poor, but it fit great and it worked perfectly for Halloween.<SEP>Rating:<2><EOS><BOS>Review: Carhartt quality and free shipping and lower prices than Tractor Supply..win win<SEP>Rating:<4><EOS><BOS>Review: Far too flimsy. As soon as the children began pretending to sweep the feathers flew off. But it will not be needed again.<SEP>Rating:<1><EOS><BOS>Review: I will need to return these. I bought size 30 waist of another brand they are fine.<SEP>Rating:<1><EOS><BOS>Review: Most pants on this size for very well...but NOT THESE.<SEP>Rating:<0><EOS><BOS>Review: much much too tight around neck, my 7 yr old grandaughter could wear it, and shes a tiny girl<SEP>Rating:<1><EOS><BOS>Review: Didn't care for reinforced knee in these, not comfortable enough for me<SEP>Rating:<2><EOS><BOS>Review: Good fit. True to size. Lightweight, comfortable material. Classic Levi styling.<SEP>Rating:<3><EOS><BOS>Review: Minutes are correct until 30 and then go backwards. Waiting on the third watch hoping this one is usable.<SEP>Rating:<0><EOS><BOS>Review: Kind of a thick and scratchy material.<SEP>Rating:<1><EOS><BOS>Review: These are nice looking boat shoes. A bit dressier than the regular ones. They fit well and run very true to size.<SEP>Rating:<4><EOS><BOS>Review: true to size. comfortable. should wear no show socks or liners until the leather softens though. perfect shoes, exactly what I had wanted.<SEP>Rating:<4><EOS><BOS>Review: sems don't hold up<SEP>Rating:<0><EOS><BOS>Review: Good shoes but get ruined fast and looks cheap<SEP>Rating:<2>PADPAD\n",
      "\n",
      "<BOS>Review: The color on the website says khaki but its more greenish then khaki and now I have to return it<SEP>Rating:<0><EOS><BOS>Review: Not fit as expected, i had to reorder 32X30 for size 30X30.<SEP>Rating:<1><EOS><BOS>Review: Fits fine, but the top edge does roll down a bit<SEP>Rating:<3><EOS><BOS>Review: a lot smaller then i imagined...<SEP>Rating:<1><EOS><BOS>Review: Aaaa<SEP>Rating:<3><EOS><BOS>Review: Extremely cheap quality, and therefore way over priced. Otherwise nice accessory to the Cowardly Lions Costume.<SEP>Rating:<1><EOS><BOS>Review: My favorite pants for 20 years<SEP>Rating:<4><EOS><BOS>Review: Ran fast. It gained about 5 minutes a day. That should not happen to a quartz watch<SEP>Rating:<1><EOS><BOS>Review: get a size bigger than you think you should<SEP>Rating:<0><EOS><BOS>Review: Very nice, happy to have them.<SEP>Rating:<3><EOS><BOS>Review: Great value. Looks nice. A little darker than the picture. Belts are never on sale so this is a winner<SEP>Rating:<3><EOS><BOS>Review: Too large for me in the thigh area. returned to Amazon. I thought the boot cut would work for me but just not the right cut.<SEP>Rating:<1><EOS><BOS>Review: ok for a child<SEP>Rating:<2><EOS><BOS>Review: Too big by order<SEP>Rating:<1><EOS><BOS>Review: These are great shoes, but they ran a little big.<SEP>Rating:<2><EOS><BOS>Review: need to see if i can swap theses for a smaller size, way too big but great condition<SEP>Rating:<2><EOS><BOS>Review: Didn't fit and i lost a lot of money paying for shipping both ways after i sent them back.<SEP>Rating:<1><EOS><BOS>Review: 874's fit better at the waist, I like that they are slightly higher than slim straight.<SEP>Rating:<3><EOS><BOS>Review: this coat is not like carhartt I purchased for my son eight years ago.it is lightweight-not warm.very disappointed in this purchase.<SEP>Rating:<1><EOS><BOS>Review: Wacoal always fits me perfectly! Great for a more natural look. Love the taupe color too. Worth the money!<SEP>Rating:<4><EOS><BOS>Review: Size didn't fit, cheaper look than the original Keds. I ended up returning the shoes.<SEP>Rating:<1><EOS><BOS>Review: Exchanged for a size larger<SEP>Rating:<1>\n",
      "\n",
      "y\n",
      "tensor([[ 7421,   292,    39,  ..., 16008, 16008, 16008],\n",
      "        [ 7421,   292,   272,  ...,   292, 16006, 16001],\n",
      "        [ 7421,   292,    39,  ..., 16008, 16008, 16008],\n",
      "        [ 7421,   292,     4,  ..., 16001, 16008, 16008],\n",
      "        [ 7421,   292,    39,  ...,   292, 16004, 16001]])\n",
      "torch.Size([5, 510])\n",
      "lengths\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x, y, lengths) in enumerate(train_loader):\n",
    "    print(\"bi\")\n",
    "    print(batch_idx)\n",
    "    print(\"x\")\n",
    "    print(x.shape)\n",
    "    xl = x[0].tolist()\n",
    "    print(xl)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(decode_seq(x[0].tolist()))\n",
    "    print()\n",
    "    print(decode_seq(x[1].tolist()))\n",
    "    print()\n",
    "    print(decode_seq(x[2].tolist()))\n",
    "    print()\n",
    "    print(decode_seq(x[3].tolist()))\n",
    "    print()\n",
    "    print(decode_seq(x[4].tolist()))\n",
    "    print()\n",
    "    print(\"y\")\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    print(\"lengths\")\n",
    "    print(lengths.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c553b33d-3abc-4f4e-96ba-c6e56fbf37bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "CUDA device count: 1\n",
      "Current CUDA device name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61b9cd69-8262-48b4-9d60-c8096fa11e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(model, optimizer, step, path):\n",
    "    ckpt = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"step\": step,\n",
    "    }\n",
    "    torch.save(ckpt, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70e18ed8-d2cc-4928-8ac3-3bdf65780839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, causal_mask, padding_mask):\n",
    "        # Self-attention (GPT-style)\n",
    "        h = self.ln1(x)\n",
    "        attn_out, _ = self.attn(\n",
    "            h, h, h,\n",
    "            attn_mask=causal_mask,\n",
    "            key_padding_mask=padding_mask,\n",
    "            need_weights=False\n",
    "        )\n",
    "        x = x + attn_out\n",
    "\n",
    "        # Feedforward\n",
    "        h = self.ln2(x)\n",
    "        ff_out = self.mlp(h)\n",
    "        x = x + ff_out\n",
    "\n",
    "        return x\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len=512,\n",
    "                 embed_dim=768, num_heads=12,\n",
    "                 num_layers=4, mlp_dim=3072, dropout=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GPTBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        #self.head.weight = self.embed.weight  # weight tying\n",
    "\n",
    "    #def causal_mask(self, T, device):\n",
    "    #    mask = torch.triu(torch.ones(T, T, device=device), 1)\n",
    "    #    return mask * float(\"-inf\")\n",
    "\n",
    "    def causal_mask(self, T, device): \n",
    "        mask = torch.triu(torch.ones(T, T, device=device), 1) \n",
    "        return mask.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        B, T = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        tok = self.embed(x)\n",
    "        pos = self.pos(torch.arange(T, device=device)[None, :])\n",
    "        h = tok + pos\n",
    "\n",
    "        causal = self.causal_mask(T, device)     # (T, T)\n",
    "        pad_mask = (x == 0)                      # (B, T)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, causal, pad_mask)\n",
    "\n",
    "        h = self.ln_final(h)\n",
    "        return self.head(h)                      # (B, T, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef8aade3-fa9b-4fcc-9bb4-cccf45b58fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d28f38d8-8731-4171-8795-44da6aca6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, epochs=10, lr=1e-4, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    track_loss = []\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        loader = tqdm(train_loader)\n",
    "        \n",
    "        for x, y, lengths in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(x, lengths)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            track_loss.append(loss.item())\n",
    "            avg_loss = sum(track_loss[-10:]) / 10\n",
    "            loader.set_postfix(loss=avg_loss)\n",
    "            del logits\n",
    "            torch.cuda.empty_cache()\n",
    "            global_step+=1\n",
    "\n",
    "            ckpt_path = f\"ckpt_{epoch}.pt\"\n",
    "            if global_step % 2500 == 0:\n",
    "                save_checkpoint(model, optimizer, global_step, ckpt_path)\n",
    "                print(f\"Saved checkpoint at step {global_step}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36b60908-11f5-444c-93ae-fd8c2568c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenDatasetB(train_seqs_x, train_seqs_y)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fnB)\n",
    "\n",
    "\n",
    "# Initialize model (set vocab_size to your tokenizer's vocab size + special tokens)\n",
    "vocab_size = 16000+9  # Adjust based on your tokenizer\n",
    "model = DecoderOnlyTransformer(vocab_size=vocab_size,  num_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a8ea1a7-ac2e-4361-acea-13086fd9662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 110039040\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47aff4d4-b6ea-49ed-83f2-6e2d656301d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:10<00:00,  9.12it/s, loss=3.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75, Average Loss: 4.4267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.06it/s, loss=3.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/75, Average Loss: 3.4482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.06it/s, loss=3.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/75, Average Loss: 3.2060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:10<00:00,  9.10it/s, loss=3.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/75, Average Loss: 3.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.06it/s, loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/75, Average Loss: 2.8571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.06it/s, loss=2.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/75, Average Loss: 2.7025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.05it/s, loss=2.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/75, Average Loss: 2.5425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.08it/s, loss=2.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/75, Average Loss: 2.3742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 100/100 [00:11<00:00,  9.03it/s, loss=2.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/75, Average Loss: 2.1897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.06it/s, loss=2.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/75, Average Loss: 2.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.06it/s, loss=1.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/75, Average Loss: 1.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 100/100 [00:11<00:00,  9.02it/s, loss=1.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/75, Average Loss: 1.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.00it/s, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/75, Average Loss: 1.4820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.07it/s, loss=1.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/75, Average Loss: 1.3258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.02it/s, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/75, Average Loss: 1.1761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 100/100 [00:11<00:00,  9.01it/s, loss=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/75, Average Loss: 1.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  8.99it/s, loss=0.957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/75, Average Loss: 0.9009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  9.04it/s, loss=0.822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/75, Average Loss: 0.7818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.03it/s, loss=0.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/75, Average Loss: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.01it/s, loss=0.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/75, Average Loss: 0.5694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  9.01it/s, loss=0.511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/75, Average Loss: 0.4798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.00it/s, loss=0.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/75, Average Loss: 0.4049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  9.06it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/75, Average Loss: 0.3374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  9.04it/s, loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/75, Average Loss: 0.2819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  8.35it/s, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 2500\n",
      "Epoch 25/75, Average Loss: 0.2367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:10<00:00,  9.18it/s, loss=0.217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/75, Average Loss: 0.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  9.05it/s, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/75, Average Loss: 0.1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:10<00:00,  9.10it/s, loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/75, Average Loss: 0.1409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:10<00:00,  9.18it/s, loss=0.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/75, Average Loss: 0.1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:10<00:00,  9.13it/s, loss=0.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/75, Average Loss: 0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.03it/s, loss=0.0921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/75, Average Loss: 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.99it/s, loss=0.0849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/75, Average Loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  9.01it/s, loss=0.074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/75, Average Loss: 0.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.00it/s, loss=0.0713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/75, Average Loss: 0.0639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.03it/s, loss=0.0639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/75, Average Loss: 0.0598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  8.92it/s, loss=0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/75, Average Loss: 0.0564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.99it/s, loss=0.0549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/75, Average Loss: 0.0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.03it/s, loss=0.0541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/75, Average Loss: 0.0498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.99it/s, loss=0.0522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/75, Average Loss: 0.0466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.98it/s, loss=0.0484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/75, Average Loss: 0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.02it/s, loss=0.0526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/75, Average Loss: 0.0464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  9.01it/s, loss=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/75, Average Loss: 0.0481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.04it/s, loss=0.0532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/75, Average Loss: 0.0494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.03it/s, loss=0.0516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/75, Average Loss: 0.0472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.00it/s, loss=0.0492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/75, Average Loss: 0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.01it/s, loss=0.0458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/75, Average Loss: 0.0428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.05it/s, loss=0.0455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/75, Average Loss: 0.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.00it/s, loss=0.0456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/75, Average Loss: 0.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.00it/s, loss=0.0447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/75, Average Loss: 0.0414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.35it/s, loss=0.0445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 5000\n",
      "Epoch 50/75, Average Loss: 0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.02it/s, loss=0.0416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/75, Average Loss: 0.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.99it/s, loss=0.0436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/75, Average Loss: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.00it/s, loss=0.0453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/75, Average Loss: 0.0396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.97it/s, loss=0.0395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/75, Average Loss: 0.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.00it/s, loss=0.0424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/75, Average Loss: 0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  8.99it/s, loss=0.039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/75, Average Loss: 0.0377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.99it/s, loss=0.0426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/75, Average Loss: 0.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.94it/s, loss=0.0413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/75, Average Loss: 0.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.98it/s, loss=0.0407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/75, Average Loss: 0.0365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:10<00:00,  9.14it/s, loss=0.0392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/75, Average Loss: 0.0356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:10<00:00,  9.19it/s, loss=0.0364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/75, Average Loss: 0.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:10<00:00,  9.11it/s, loss=0.0389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/75, Average Loss: 0.0348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:10<00:00,  9.10it/s, loss=0.0372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/75, Average Loss: 0.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  9.09it/s, loss=0.0367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/75, Average Loss: 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:10<00:00,  9.11it/s, loss=0.037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/75, Average Loss: 0.0337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:10<00:00,  9.15it/s, loss=0.0332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/75, Average Loss: 0.0327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:10<00:00,  9.12it/s, loss=0.033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/75, Average Loss: 0.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:10<00:00,  9.13it/s, loss=0.0324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/75, Average Loss: 0.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:10<00:00,  9.12it/s, loss=0.0352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/75, Average Loss: 0.0316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:11<00:00,  9.09it/s, loss=0.036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/75, Average Loss: 0.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:10<00:00,  9.10it/s, loss=0.0369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/75, Average Loss: 0.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:10<00:00,  9.16it/s, loss=0.0342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/75, Average Loss: 0.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 100/100 [00:10<00:00,  9.11it/s, loss=0.036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/75, Average Loss: 0.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 100/100 [00:11<00:00,  9.08it/s, loss=0.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/75, Average Loss: 0.0297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 100/100 [00:11<00:00,  8.47it/s, loss=0.0365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 7500\n",
      "Epoch 75/75, Average Loss: 0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = train_model(model, train_loader, epochs=75, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "2a407e8d-06a2-47be-aab5-93aaa166bfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOnlyTransformer(\n",
       "  (embed): Embedding(16009, 768, padding_idx=0)\n",
       "  (pos): Embedding(512, 768)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x GPTBlock(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=768, out_features=16009, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "e21d8c29-ba36-41fe-9509-a107481bfbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [345, 23, 44, 99]\n",
    "kt = torch.tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "65c02368-8230-4a74-b928-61157375ed3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "dcc225f7-a78e-4020-9977-d1fb388eb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno, score = getEncodingOpen(test_df, random.randint(0,len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "bbb9608b-ed31-42d0-a4cd-b94fdbe089c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53772/4078374235.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  annot = torch.tensor(anno)\n"
     ]
    }
   ],
   "source": [
    "annot = torch.tensor(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "2ca34894-ec80-4b12-ba7e-3bbaad8b7605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS>Review: Perfect.<SEP>Rating:'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(anno.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "f690c9bb-2855-4205-a7a4-0ace3d698c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16000,  7421,   292,   269,     3, 16002, 13254,   292])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "366e1343-ffe1-437c-986d-0c836039f334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "afdf5a43-2b53-4a28-b6d9-623da3bcd233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53772/1037303179.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_x[0,:lens] = torch.tensor(anno)\n"
     ]
    }
   ],
   "source": [
    "lens = len(anno)\n",
    "anno_len = torch.tensor([lens])\n",
    "\n",
    "# Pad sequences\n",
    "padded_x = torch.zeros(1, 512, dtype=torch.long) + pad_token\n",
    "padded_x[0,:lens] = torch.tensor(anno)\n",
    "\n",
    "padded_x = padded_x.to(device)\n",
    "anno_len = anno_len.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "31be58a7-a52f-4ae5-ace2-6794c1b57754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "4c49ebd3-5b50-43d0-86d2-69d2c1f68576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS>Review: Perfect.<SEP>Rating:PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(padded_x[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8c3453a3-b20d-46b8-874f-6afb88d26fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(padded_x, anno_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "37a76155-a067-4e39-817e-3312373d71ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 16009])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "9f36c7b5-4122-4c48-8bef-970b56dc5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenout = torch.argmax(out,dim=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "77190548-0dfa-4912-9462-ccd4e0f5e6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "3f9a6132-6236-43fc-8b02-22ea37a78894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "0dada6bd-507c-4bf3-90d5-53036f14bd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Review: I for<SEP>Rating:<4>PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADofPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD'"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(tokenout.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3016a3-6e0d-48cd-afaa-61187880b560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "54d0ba14-4687-46de-a45c-49d702ce45e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_probs: [[2.4788746e-05 3.9458470e-07 1.9498410e-05 7.6452428e-03 9.9216765e-01]]\n",
      "top5 ids: [[16007 16006 16003 16005   193]]\n",
      "top1 ids: [[16007]]\n"
     ]
    }
   ],
   "source": [
    "def predict_label_ids(model, x, pad_token):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)                    # (B, T, V)\n",
    "        nonpad_len = (x != pad_token).sum(dim=1)   # (B,)\n",
    "        last_pos = nonpad_len - 1\n",
    "        last_logits = logits[torch.arange(x.size(0)), last_pos]   # (B, V)\n",
    "        probs = last_logits.softmax(dim=-1)\n",
    "        top5 = probs.topk(5, dim=-1)\n",
    "        top1 = probs.topk(1, dim = -1)\n",
    "        return top5, probs[:, 16003:16008], top1   # top5 and probs specifically for label tokens\n",
    "\n",
    "# Example:\n",
    "#x = torch.LongTensor([padded_x]).to(device)\n",
    "top5, label_probs, top1 = predict_label_ids(model, padded_x, pad_token)\n",
    "print(\"label_probs:\", label_probs.cpu().numpy())\n",
    "print(\"top5 ids:\", top5.indices.cpu().numpy())\n",
    "print(\"top1 ids:\", top1.indices.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e747a5bf-22d2-422c-b001-ca190ee73577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<4>', 4)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(top1[1].tolist()[0]), score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "056021ea-c2ee-4b3b-83a1-a39e04cb3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLDataset(Dataset):\n",
    "    def __init__(self, df, shots, seq_len):\n",
    "        self.df = df\n",
    "        self.shots = shots\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def getpre(self, idx, l):\n",
    "        for _ in range(10):\n",
    "            ixs = []\n",
    "            xs = []\n",
    "            for j in range(self.shots):\n",
    "                sel = idx\n",
    "                while(sel==idx):\n",
    "                    sel = random.randint(0, self.__len__() -1)\n",
    "                xs.extend(getEncoding(self.df, sel))\n",
    "            if(len(xs) + l <= seq_len):\n",
    "                return torch.LongTensor(xs)\n",
    "        raise ValueError(f\"can't fit {self.shots} examples in context\")\n",
    "                \n",
    "            \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = getEncodingOpen(self.df, idx)\n",
    "        l = len(x)\n",
    "        pre = self.getpre(idx, l)\n",
    "        icl_x = torch.cat((pre,x))\n",
    "        return icl_x, y\n",
    "\n",
    "# Padding collate function for variable length sequences\n",
    "def collate_fn_icl(batch):\n",
    "    seqs_x, y = zip(*batch)\n",
    "    lens = [len(s) for s in seqs_x]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_x = torch.zeros(len(seqs_x), max_len, dtype=torch.long) + pad_token\n",
    "    \n",
    "    for i, x in enumerate(seqs_x):\n",
    "        padded_x[i, :len(x)] = x\n",
    "    \n",
    "    return padded_x, torch.LongTensor(y), torch.LongTensor(lens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "4a205d72-f62a-4c03-8f30-01030aebbc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_dataset = ICLDataset(train_df, 0, seq_len)\n",
    "icl_loader = DataLoader(icl_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_icl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "2cefcec9-c47a-4681-9065-2b98bd131630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi\n",
      "0\n",
      "x\n",
      "torch.Size([8, 27])\n",
      "[16000, 7421, 292, 39, 278, 99, 1600, 547, 94, 5, 748, 1000, 152, 115, 594, 14, 11, 4424, 128, 774, 3, 16002, 13254, 292, 16008, 16008, 16008]\n",
      "\n",
      "<BOS>Review: The leather has pulled away from the chain purse after only using it for (1) month.<SEP>Rating:PADPADPAD\n",
      "y\n",
      "tensor([0, 3, 4, 1, 2, 0, 1, 1])\n",
      "torch.Size([8])\n",
      "lengths\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x, y, lengths) in enumerate(icl_loader):\n",
    "    print(\"bi\")\n",
    "    print(batch_idx)\n",
    "    print(\"x\")\n",
    "    print(x.shape)\n",
    "    xl = x[0].tolist()\n",
    "    print(xl)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(decode_seq(xl))\n",
    "    print(\"y\")\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    print(\"lengths\")\n",
    "    print(lengths.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "884c4939-b40f-4f44-a047-71c0b6e6da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def israting(s):\n",
    "    bnk = [\"0\",\"1\",\"2\",\"3\",\"4\"]\n",
    "    return s in bnk\n",
    "def isnum(s):\n",
    "    bnk = [str(n) for n in range(0,20)]\n",
    "    return s in bnk\n",
    "\n",
    "def check_token_list(token_list):\n",
    "    isratings = 0\n",
    "    isnums = 0\n",
    "    for token in token_list:\n",
    "        isratings += 1 if israting(token) else 0\n",
    "        isnums += 1 if isnum(token) else 0\n",
    "    return isratings, isnums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "12cdc769-40a8-447f-86e1-272ae6ce03bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16000: '<BOS>',\n",
       " 16001: '<EOS>',\n",
       " 16002: '<SEP>',\n",
       " 16003: '<0>',\n",
       " 16004: '<1>',\n",
       " 16005: '<2>',\n",
       " 16006: '<3>',\n",
       " 16007: '<4>',\n",
       " 16008: 'PAD'}"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "7380fe5d-f0f6-4cea-9491-67dc979a59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(x):\n",
    "    x = x.tolist()\n",
    "    for i in range(len(x)-1,0, -1):\n",
    "        if 16003<= x[i] <=16007:\n",
    "            return x[i]\n",
    "    return 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "014392b4-ea3b-4882-879f-5ede3409a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ex = torch.tensor([ 7421,   292,     4,     6,   945,    35,   161,    19,    31,   161,\n",
    "             6,    18,   161,    19,    10,    29,    26,   143,   204,     3,\n",
    "            28,   321,    14,   143,    75,   460, 13254,   292, 16000, 16008,\n",
    "         16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008,\n",
    "         16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008,\n",
    "         16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008,\n",
    "         16008])\n",
    "vv = get_score(test_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "06cd2534-def8-4717-961a-303c3ad4801a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "1c91d546-ca7f-4e19-ae3b-43561b358d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokenScore(t):\n",
    "    if(t==20000):\n",
    "        return '-'\n",
    "    return d[t][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c8caae84-5041-4f0f-a364-73ec24e13b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_dataset = ICLDataset(train_df, 0, seq_len)\n",
    "icl_loader = DataLoader(icl_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_icl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "755ae39c-767c-4cd6-b79a-d9f814cd9061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 29])\n",
      "tensor([16000,  7421,   292,   266,    35,    49,    35,    18,    98,  4521,\n",
      "           15,    23,   140,    47, 16002, 13254,   292, 16008, 16008, 16008,\n",
      "        16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008, 16008],\n",
      "       device='cuda:0')\n",
      "<BOS>Review: Not as comfortable as my other sauconys but still good<SEP>Rating:PADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "<BOS>Review: Boots came in PERFECT condition and fit just as expected<SEP>Rating:PADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "<BOS>Review: I bought dickies before & were knee length with 13\" inseam these are halfway down my calf,will return them!<SEP>Rating:\n",
      "\n",
      "<BOS>Review: It's ok.<SEP>Rating:PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "<BOS>Review: Love them!<SEP>Rating:PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "<BOS>Review: It was not a bad fit and worked out well. I will order other trousers in the near future.<SEP>Rating:PADPAD\n",
      "\n",
      "<BOS>Review: Poor quality, it leaked the first time I inflated it.<SEP>Rating:PADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "<BOS>Review: Too tight across top of shoe .... returned<SEP>Rating:PADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "========================\n",
      "Review: I really thrilled as the Skamp, S liked I looks enough Rating:<3>PADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "Review: I to in the or. then well these expected. Rating:<2>PADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "Review: I ordered these shoes, day to in of a!! waist' shoes great would. husband and however men them.<SEP>Rating:<4>\n",
      "\n",
      "Review: I's good but Not Rating:<3>PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "Review: I the!<SEP>Rating:<4>PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "Review: I' OK filled gift for. fit shoes of, I bought break large Dickies, great mostly/<SEP>Rating:<2>PADPAD\n",
      "\n",
      "Review: Ily, which was true texture time. have weight without<SEP>Rating:<3>PADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "Review: I small. the fit my I and it Rating:<0>PADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "\n",
      "[16006, 16005, 16007, 16006, 16007, 16005, 16006, 16003]\n",
      "---\n",
      "['3', '2', '4', '3', '4', '2', '3', '0']\n",
      "['2', '4', '1', '2', '3', '3', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "num = 0\n",
    "ratings = 0\n",
    "correct = 0\n",
    "for batch_idx, (x, y, lengths) in enumerate(icl_loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    pred_logits = model(x,lengths)\n",
    "    print(x.shape)\n",
    "    print(x[0])\n",
    "    for sequence in x:\n",
    "        print(decode_seq(sequence.tolist()))\n",
    "        print()\n",
    "    print(\"========================\")\n",
    "    pred_tokens = torch.argmax(pred_logits,dim=2)\n",
    "    pred_tokens = pred_tokens.to('cpu')\n",
    "    #pred_tokens = trim_tail(pred_tokens, pad_token)\n",
    "    #pred_last_token = pred_tokens[:,-1].tolist()\n",
    "    pred_last_token = [get_score(i) for i in pred_tokens]\n",
    "    for sequence in pred_tokens:\n",
    "        print(decode_seq(sequence.tolist()))\n",
    "        print()\n",
    "    pred_scores = []\n",
    "    print(pred_last_token)\n",
    "    for token in pred_last_token:\n",
    "        if(token<=15999):\n",
    "            pred_scores.append(tok.decode([token]))\n",
    "        else:\n",
    "            pred_scores.append(getTokenScore(token))\n",
    "\n",
    "    print('---')\n",
    "    print(pred_scores)\n",
    "    \n",
    "    ys = [str(ans) for ans in y.tolist()]\n",
    "    print(ys)\n",
    "    tot += len(y)\n",
    "    for t in range(len(y)):\n",
    "        correct += 1 if pred_scores[t] == ys[t] else 0\n",
    "    isratings, isnums = check_token_list(pred_scores)\n",
    "    num += isnums\n",
    "    ratings += isratings\n",
    "    torch.cuda.empty_cache()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f502f9f4-9814-46d8-9f57-75d87a636bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot:  8\n",
      "num:  8\n",
      "ratings:  8\n",
      "correct:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"tot: \", tot)\n",
    "print(\"num: \", num)\n",
    "print(\"ratings: \", ratings)\n",
    "print(\"correct: \", correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "4321cea4-bb42-4134-88bf-b6f6411167ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_dataset = ICLDataset(test_df, 5, seq_len)\n",
    "icl_loader = DataLoader(icl_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_icl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "c125a0ff-bbbf-4133-bbe7-93e96cb0bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_ans = []\n",
    "pred_ans = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "68778e4c-f65d-426b-8099-ac08645be7eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.55 GiB of which 8.31 MiB is free. Including non-PyTorch memory, this process has 4.64 GiB memory in use. Process 56626 has 10.46 GiB memory in use. Of the allocated memory 3.87 GiB is allocated by PyTorch, and 474.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[385], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m lengths \u001b[38;5;241m=\u001b[39m lengths\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m pred_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#print(x[0])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#for sequence in x:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#    print(decode_seq(sequence.tolist()))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#    print()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#print(\"========================\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m pred_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(pred_logits,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/berkeley/cs182/cs182/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/berkeley/cs182/cs182/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[41], line 77\u001b[0m, in \u001b[0;36mDecoderOnlyTransformer.forward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m     74\u001b[0m pad_mask \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)                      \u001b[38;5;66;03m# (B, T)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 77\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(h)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(h)\n",
      "File \u001b[0;32m~/Desktop/berkeley/cs182/cs182/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/berkeley/cs182/cs182/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[41], line 22\u001b[0m, in \u001b[0;36mGPTBlock.forward\u001b[0;34m(self, x, causal_mask, padding_mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, causal_mask, padding_mask):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Self-attention (GPT-style)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[0;32m---> 22\u001b[0m     attn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Feedforward\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/berkeley/cs182/cs182/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/berkeley/cs182/cs182/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/berkeley/cs182/cs182/lib/python3.10/site-packages/torch/nn/modules/activation.py:1488\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1463\u001b[0m         query,\n\u001b[1;32m   1464\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1486\u001b[0m     )\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/Desktop/berkeley/cs182/cs182/lib/python3.10/site-packages/torch/nn/functional.py:6307\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   6304\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   6305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6306\u001b[0m     )\n\u001b[0;32m-> 6307\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6309\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   6310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6311\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/berkeley/cs182/cs182/lib/python3.10/site-packages/torch/nn/functional.py:5706\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   5699\u001b[0m     proj \u001b[38;5;241m=\u001b[39m linear(q, w, b)\n\u001b[1;32m   5700\u001b[0m     \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   5701\u001b[0m     proj \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   5702\u001b[0m         \u001b[43mproj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5703\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5704\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m-> 5706\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5707\u001b[0m     )\n\u001b[1;32m   5708\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proj[\u001b[38;5;241m0\u001b[39m], proj[\u001b[38;5;241m1\u001b[39m], proj[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   5709\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5710\u001b[0m     \u001b[38;5;66;03m# encoder-decoder attention\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.55 GiB of which 8.31 MiB is free. Including non-PyTorch memory, this process has 4.64 GiB memory in use. Process 56626 has 10.46 GiB memory in use. Of the allocated memory 3.87 GiB is allocated by PyTorch, and 474.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "num = 0\n",
    "ratings = 0\n",
    "correct = 0\n",
    "for batch_idx, (x, y, lengths) in enumerate(icl_loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    pred_logits = model(x,lengths)\n",
    "    #print(x.shape)\n",
    "    #print(x[0])\n",
    "    #for sequence in x:\n",
    "    #    print(decode_seq(sequence.tolist()))\n",
    "    #    print()\n",
    "    #print(\"========================\")\n",
    "    pred_tokens = torch.argmax(pred_logits,dim=2)\n",
    "    pred_tokens = pred_tokens.to('cpu')\n",
    "    #pred_tokens = trim_tail(pred_tokens, pad_token)\n",
    "    #pred_last_token = pred_tokens[:,-1].tolist()\n",
    "    pred_last_token = [get_score(i) for i in pred_tokens]\n",
    "    #for sequence in pred_tokens:\n",
    "    #    print(decode_seq(sequence.tolist()))\n",
    "    #    print()\n",
    "    pred_scores = []\n",
    "    #print(pred_last_token)\n",
    "    for token in pred_last_token:\n",
    "        if(token<=15999):\n",
    "            pred_scores.append(tok.decode([token]))\n",
    "        else:\n",
    "            pred_scores.append(getTokenScore(token))\n",
    "\n",
    "    #print('---')\n",
    "    #print(pred_scores)\n",
    "    ys = [str(ans) for ans in y.tolist()]\n",
    "\n",
    "    pred_ans.extend(pred_scores)\n",
    "    gt_ans.extend(ys)\n",
    "    #print(ys)\n",
    "    tot += len(y)\n",
    "    for t in range(len(y)):\n",
    "        correct += 1 if pred_scores[t] == ys[t] else 0\n",
    "    isratings, isnums = check_token_list(pred_scores)\n",
    "    num += isnums\n",
    "    ratings += isratings\n",
    "    del pred_logits\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392f1a0-5382-4455-9c0a-6671064f7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tot: \", tot)\n",
    "print(\"num: \", num)\n",
    "print(\"ratings: \", ratings)\n",
    "print(\"correct: \", correct)\n",
    "print(\"acc: \", correct/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f018b-dc02-4e6e-a35e-a0ea7a60ac98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "d9b30555-422e-4601-bc3a-178ede39e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = [(int(gt_ans[i]) - int(pred_ans[i]))**2 for i in range(len(gt_ans))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "f151cbb1-3399-438a-a174-8ff17d0417aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE\n",
      "2.9982608695652173\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE\")\n",
    "print(sum(diffs)/len(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78220f0-c788-4320-9ee4-3f419734acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "5358/15387 #0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ac781-88b8-4897-926b-2ed66d48c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    " 4778 / 15396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fd914-8d64-45f4-afb8-2cece4e8d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473732a9-db08-4199-9b61-11a7d9685629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
